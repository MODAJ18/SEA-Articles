{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa97ceac",
   "metadata": {},
   "source": [
    "- a software system to bring out information of web articles from consiting of articles and blogs\n",
    "\n",
    "- smart search engines can be considered unsupervised learning approaches, due to the nature of clustering related information without such label in hand\n",
    "\n",
    "- Search Engines have evolved **from a text input and output service to** an experience that cuts across voice, video, documents, and conversations\n",
    "\n",
    "\n",
    "- an **infinite problem** to solve\n",
    "\n",
    "\n",
    "- **related** to information retrieval, language understanding\n",
    "\n",
    "\n",
    "- the **value that an effective search tool can bring to a business is enormous**; a key piece of intellectual property. Often a search bar is the main interface between customers and the business. \n",
    "    - create a competitive advantage by delivering an improved user experience.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1a2c1f",
   "metadata": {},
   "source": [
    "search engine popular approaches:\n",
    "- manual implementation with dataframe + tf-idf\n",
    "- Elastic Search + BM25\n",
    "- BM25 + Azure Cognitive Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40924fed",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "- Search index for storing each document, reflecting relevant information and up to date information\n",
    "    - data can be reorganized by date (suggestion)\n",
    "- Query understanding\n",
    "    - takes sentence and preprocessed data information **directly without much context**\n",
    "    - we can extract words or tokens from the query to match **article_type** (suggestion)\n",
    "        - query to match tags (done)\n",
    "    - we can filter the search by either blog or News (suggestion)\n",
    "        - or add multiple results available (blog, News, or both)\n",
    "    - BM25 + Azure Cognitive search\n",
    "- Query ranking\n",
    "    - by consine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f41a6",
   "metadata": {},
   "source": [
    "## 1- Library and Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5fd3ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# for text cleaning and preprocessing\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e2ec345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.read_json('../Data/husna.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f9280",
   "metadata": {},
   "source": [
    "## 2- Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3dccb",
   "metadata": {},
   "source": [
    "#### 2.1 preparing data for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5990710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "docs_df = docs_df.drop(columns=['publisher', 'crawled_at', 'published_at'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f6134b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df_dropped = docs_df.drop(index=\n",
    "                               docs_df[(docs_df['content'].str.len() == 0) & (docs_df['title'] == '')].index, axis=0)\n",
    "docs_df_dropped = docs_df_dropped.reset_index(drop=True)\n",
    "docs_df = docs_df_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "00f3d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df['text'] = docs_df['content'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81137f2",
   "metadata": {},
   "source": [
    "## 3- Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c326af",
   "metadata": {},
   "source": [
    "important data cleaning functions:\n",
    "- remove punctuation\n",
    "- tokenization \n",
    "- stem words\n",
    "\n",
    "**cleaning functions not implemented**: removing repeating characters, stop words, emoji, hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b289ab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_info_text(df_col):\n",
    "    print(f\"-> Number of Documents: {docs_df.shape[0]}\")\n",
    "    print('-' * 50, end='\\n\\n')\n",
    "\n",
    "    print('-> Documents - First 150 letters')\n",
    "    print()\n",
    "    for i, document_i in enumerate(docs_df['text_clean'][:20]):\n",
    "        print(f\"Document Number {i+1}: {document_i[:150]}..\")\n",
    "        print()\n",
    "\n",
    "    print('-' * 50)\n",
    "    \n",
    "def data_preprocessing(df_col):\n",
    "    # Instantiate a TfidfVectorizer object\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # It fits the data and transform it as a vector\n",
    "    X = vectorizer.fit_transform(df_col)\n",
    "    # Convert the X as transposed matrix\n",
    "    X = X.T.toarray()\n",
    "    # Create a DataFrame and set the vocabulary as the index\n",
    "    df = pd.DataFrame(X, index=vectorizer.get_feature_names())\n",
    "    return df, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340c31f",
   "metadata": {},
   "source": [
    "### 3.1 data cleaning (ver.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392400ab",
   "metadata": {},
   "source": [
    "handle:\n",
    "- removing mentions\n",
    "- removing punctuation\n",
    "- removing Arabic diacritics (short vowels and other harakahs) \n",
    "    - حركات وشد\n",
    "- removing elongation \n",
    "    - مد\n",
    "- removing stopwords (which is available in NLTK corpus)\n",
    "    - normal stopwords (not specific to arabic)\n",
    "- remove words from languages other than arabic and english"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8c63f",
   "metadata": {},
   "source": [
    "**NOTE** \n",
    "- ~6000 source documents -> ~5000 documents -> ~89,000 tokens\n",
    "- Clean time for 'text' column: ~111 seconds\n",
    "- **Problems**:\n",
    "    - may not be normalized enough\n",
    "    - words from other languages\n",
    "    - confusing numbers (remove or keep?)\n",
    "        - remove english numbers? or arabic numbers? or both?\n",
    "        - should we remove words of letters mixed with numbers (E.g. COVID19)\n",
    "    - links (remove or keep?)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f88040c",
   "metadata": {},
   "source": [
    "### 3.2 data cleaning (ver.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a35f97",
   "metadata": {},
   "source": [
    "handle:\n",
    "- removing Arabic diacritics (short vowels and other harakahs)\n",
    "- variation by form and spelling, based on context (Orthographic Ambiguity)\n",
    "- existence of many forms for the same word (Morphological Richness)\n",
    "- dialects (Dialectal Variation)\n",
    "- different ways to write the same word when writing in dialectal Arabic, for which there is no agreed-upon standard\n",
    "    - Orthographic Inconsistency\n",
    "- removing longation and stop words\n",
    "- remove words from languages other than arabic and english\n",
    "   \n",
    "these problems can possibly lead to immensly large vocabularies generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "adfcff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df_cleaned2 = docs_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "31a52569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dediacritization tool\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "# Reducing Orthographic Ambiguity\n",
    "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_ar\n",
    "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
    "\n",
    "# toknenization\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "\n",
    "# Morphological Disambiguation (Maximum Likelihood Disambiguator)\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "mle = MLEDisambiguator.pretrained() # instantiation fo MLE disambiguator\n",
    "\n",
    "# tokenization / lemmatization (choosing approach that best fit the project)\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7049ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_list = pd.read_csv('../Data/stop_words/list.csv')['words'].to_list()\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='atbtok', diac=False) # atbseg scheme \n",
    "def text_clean2(txt):\n",
    "    # remove stopwords\n",
    "    txt = ' '.join(word for word in txt.split() if word not in stop_word_list)\n",
    "    \n",
    "    # dediacritization\n",
    "    txt = dediac_ar(txt)\n",
    "    \n",
    "    # normalization: Reduce Orthographic Ambiguity and Dialectal Variation\n",
    "    txt = normalize_alef_maksura_ar(txt)\n",
    "    txt = normalize_alef_ar(txt)\n",
    "    txt = normalize_teh_marbuta_ar(txt)\n",
    "    \n",
    "    # normalization: Reducing Morphological Variation\n",
    "    tokens = simple_word_tokenize(txt)\n",
    "    disambig = mle.disambiguate(tokens)\n",
    "    lemmas = [d.analyses[0].analysis['lex'] for d in disambig]\n",
    "    tokens = tokenizer.tokenize(lemmas)\n",
    "    txt = ' '.join(tokens)\n",
    "    \n",
    "    # remove longation\n",
    "    txt = re.sub(\"[إأآا]\", \"ا\", txt)\n",
    "    txt = re.sub(\"ى\", \"ي\", txt)\n",
    "    txt = re.sub(\"ؤ\", \"ء\", txt)\n",
    "    txt = re.sub(\"ئ\", \"ء\", txt)\n",
    "    txt = re.sub(\"ة\", \"ه\", txt)\n",
    "    txt = re.sub(\"گ\", \"ك\", txt)\n",
    "    \n",
    "    # remove non-arabic words, or non-numbers, or non-english words in the text\n",
    "    txt = re.sub(r'[^a-zA-Z\\s0-9\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD.0-9]+'\n",
    "                 ,' ', txt)\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b3c04a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to your text column\n",
    "docs_df_cleaned2 = docs_df.drop(columns=['_id', 'summary', 'content'])\n",
    "start_time = time.time()\n",
    "docs_df_cleaned2['text_clean'] = docs_df['text'].apply(text_clean2)\n",
    "docs_df_cleaned2['title_clean'] = docs_df['title'].apply(text_clean2)\n",
    "time_measure = (time.time() - start_time) * 10**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "87b111f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df_cleaned2['content_clean'] = docs_df_cleaned2['title_clean'] + \" \" + docs_df_cleaned2['text_clean']\n",
    "docs_df_cleaned2['doc_id'] = docs_df_cleaned2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1d997087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>tags</th>\n",
       "      <th>article_type</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>التربية: تحويل 42 مدرسة إلى نظام الفترتين واستئجار 15 مدرسة هذا العام</td>\n",
       "      <td>https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%A7%D9%84%D8%AA%D8%B1%D8%A8%D9%...</td>\n",
       "      <td>[التربية والتعليم, وزارة التربية والتعليم]</td>\n",
       "      <td>News</td>\n",
       "      <td>أكدت أمين عام  وزارة  التربية والتعليم للشؤون المالية والإدارية الدكتورة نجو...</td>\n",
       "      <td>اكد امين وزاره تربيه تعليم شان مالي اداري دكتور نجوي القبيلات ان توسع دوام ط...</td>\n",
       "      <td>تربيه   تحويل 42 مدرسه الي نظام فتره استءجار 15 مدرسه عام</td>\n",
       "      <td>تربيه   تحويل 42 مدرسه الي نظام فتره استءجار 15 مدرسه عام اكد امين وزاره ترب...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تكريما للمعلمين زيادة منح أبناء المعلمين 550 مقعدا إضافيا بالجامعات</td>\n",
       "      <td>https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%B2%D9%8A%D8%A7%D8%AF%D8%A9-%D9...</td>\n",
       "      <td>[مكرمة أبناء المعلمين, وزارة التربية والتعليم]</td>\n",
       "      <td>News</td>\n",
       "      <td>احتفلت  وزارة التربية والتعليم  بيوم المعلم بتكريمها نخبة من المعلمات والمعل...</td>\n",
       "      <td>احتفل وزاره تربيه تعليم يوم معلم تكريم نخبه معلم معلم مختلف مديريه تربيه تعل...</td>\n",
       "      <td>تكريم معلم زياده منح ابناء معلم 550 مقعد اضافي جامعه</td>\n",
       "      <td>تكريم معلم زياده منح ابناء معلم 550 مقعد اضافي جامعه احتفل وزاره تربيه تعليم...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   title  \\\n",
       "0  التربية: تحويل 42 مدرسة إلى نظام الفترتين واستئجار 15 مدرسة هذا العام   \n",
       "1    تكريما للمعلمين زيادة منح أبناء المعلمين 550 مقعدا إضافيا بالجامعات   \n",
       "\n",
       "                                                                               url  \\\n",
       "0  https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%A7%D9%84%D8%AA%D8%B1%D8%A8%D9%...   \n",
       "1  https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%B2%D9%8A%D8%A7%D8%AF%D8%A9-%D9...   \n",
       "\n",
       "                                             tags article_type  \\\n",
       "0      [التربية والتعليم, وزارة التربية والتعليم]         News   \n",
       "1  [مكرمة أبناء المعلمين, وزارة التربية والتعليم]         News   \n",
       "\n",
       "                                                                              text  \\\n",
       "0  أكدت أمين عام  وزارة  التربية والتعليم للشؤون المالية والإدارية الدكتورة نجو...   \n",
       "1  احتفلت  وزارة التربية والتعليم  بيوم المعلم بتكريمها نخبة من المعلمات والمعل...   \n",
       "\n",
       "                                                                        text_clean  \\\n",
       "0  اكد امين وزاره تربيه تعليم شان مالي اداري دكتور نجوي القبيلات ان توسع دوام ط...   \n",
       "1  احتفل وزاره تربيه تعليم يوم معلم تكريم نخبه معلم معلم مختلف مديريه تربيه تعل...   \n",
       "\n",
       "                                                 title_clean  \\\n",
       "0  تربيه   تحويل 42 مدرسه الي نظام فتره استءجار 15 مدرسه عام   \n",
       "1       تكريم معلم زياده منح ابناء معلم 550 مقعد اضافي جامعه   \n",
       "\n",
       "                                                                     content_clean  \\\n",
       "0  تربيه   تحويل 42 مدرسه الي نظام فتره استءجار 15 مدرسه عام اكد امين وزاره ترب...   \n",
       "1  تكريم معلم زياده منح ابناء معلم 550 مقعد اضافي جامعه احتفل وزاره تربيه تعليم...   \n",
       "\n",
       "   doc_id  \n",
       "0       0  \n",
       "1       1  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df_cleaned2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f10f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_df_cleaned2.to_csv('../Data/processed/SE_data4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1fdde630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time measure:  1535.5424880981445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modaj\\OneDrive\\Documents\\Personal\\Jobs\\SHAI\\intern - task 3\\haystack-venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# fit + transform time\n",
    "\n",
    "start_time = time.time()\n",
    "text_clean_enc_df, clean2_vect = data_preprocessing(docs_df_cleaned2['text_clean'])\n",
    "# text_clean_enc_df = clean2_vect.transform(docs_df_cleaned2['text_clean'])\n",
    "time_measure = (time.time() - start_time) * 10**3\n",
    "\n",
    "print('time measure: ', time_measure)\n",
    "# text_clean_enc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13851cf6",
   "metadata": {},
   "source": [
    "- ~6000 source documents -> ~5000 documents -> ~23,000 tokens\n",
    "- Clean time for 'text' column: ~180 seconds\n",
    "- Problems:\n",
    "    - stopwords (remove or keep?)\n",
    "    - normalization may have cut out too many tokens \n",
    "    - confusing numbers (remove or keep?)\n",
    "        - remove english numbers? or arabic numbers? or both?\n",
    "    - should we remove words of letters mixed with numbers (E.g. COVID19)\n",
    "    - links (remove or keep?)\n",
    "    \n",
    "**NOTE** discuss with instructor before proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82637c",
   "metadata": {},
   "source": [
    "### 3.3 check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "21919998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(docs_df_cleaned.head())\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e729eb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> 37\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['732811', '733', '7331', '734', '734811', '735', '7354', '73600', '737',\n",
       "       '7375659', '737888', '738', '7383', '739', '739015', '73997', '74',\n",
       "       '7407053', '741', '7413', '742521', '743', '743331', '74413', '745667',\n",
       "       '746', '7469', '747', '748', '749046', '74915', '75', '750', '7500',\n",
       "       '7508948', '751', '7520', '753', '754', '755', '7565', '758', '75921',\n",
       "       '75zdsxi5bf', '76', '760', '76003', '761', '762', '763'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['هجمه', 'هجهوج', 'هجو', 'هجوبا', 'هجوم', 'هجوي', 'هجي', 'هجين', 'هد',\n",
       "       'هدا', 'هداء', 'هدار', 'هداريم', 'هداف', 'هدام', 'هدايه', 'هدد', 'هدر',\n",
       "       'هدرا', 'هدف', 'هدم', 'هدنه', 'هدهد', 'هدوء', 'هدوءا', 'هدول', 'هدي',\n",
       "       'هديب', 'هدير', 'هديل', 'هديه', 'هذ', 'هذا', 'هذلول', 'هراء', 'هراوه',\n",
       "       'هرب', 'هرتزليا', 'هرتسليا', 'هرتسوج', 'هرتسوغ', 'هرس', 'هرسك', 'هرش',\n",
       "       'هرطقات', 'هرطقه', 'هرع', 'هرف', 'هرقل'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean time: 1.54 seconds\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print(f'--> {i}')\n",
    "display(text_clean_enc_df.index[50*i:50*(i+1)])\n",
    "display(text_clean_enc_df.index[-50*(i+1):(-50*i)-1])\n",
    "i += 1\n",
    "\n",
    "print('clean time: {:.2f} seconds'.format(time_measure * 10**-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890fd90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ = vectorizer.vocabulary_\n",
    "print(f\"number of unique words: {len(vocab_.keys())}\")\n",
    "most_freq_word = sorted(vocab_.items(), key=lambda x: x[1], reverse=True)[:1][0]\n",
    "print('most frequent word is --> {} ({} times)'.format(most_freq_word[0], most_freq_word[1]))\n",
    "score = len(vocab_.keys()) / most_freq_word[1]\n",
    "print('Ratio: {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d668f587",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff5721",
   "metadata": {},
   "source": [
    "## 4- Apply Cleaning on Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5b141063",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_list = pd.read_csv('../Data/stop_words/list.csv')['words'].to_list()\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='atbtok', diac=False) # atbseg scheme \n",
    "def text_clean2_steps(txt):\n",
    "    # remove stopwords\n",
    "    txt = ' '.join(word for word in txt.split() if word not in stop_word_list)\n",
    "    print('stopwords', txt)\n",
    "    \n",
    "    # dediacritization\n",
    "    txt = dediac_ar(txt)\n",
    "    print('dediacritization', txt)\n",
    "    \n",
    "    # normalization: Reduce Orthographic Ambiguity and Dialectal Variation\n",
    "    txt = normalize_alef_maksura_ar(txt)\n",
    "    print('Reduce Orthographic Ambiguity and Dialectal Variation', txt)\n",
    "    txt = normalize_alef_ar(txt)\n",
    "    print('Reduce Orthographic Ambiguity and Dialectal Variation', txt)\n",
    "    txt = normalize_teh_marbuta_ar(txt)\n",
    "    print('Reduce Orthographic Ambiguity and Dialectal Variation', txt)\n",
    "    \n",
    "    # normalization: Reducing Morphological Variation\n",
    "    tokens = simple_word_tokenize(txt)\n",
    "    disambig = mle.disambiguate(tokens)\n",
    "    lemmas = [d.analyses[0].analysis['lex'] for d in disambig]\n",
    "    tokens = tokenizer.tokenize(lemmas)\n",
    "    txt = ' '.join(tokens)\n",
    "    print('Reducing Morphological Variation', txt)\n",
    "    \n",
    "    # remove longation\n",
    "    txt = re.sub(\"[إأآا]\", \"ا\", txt)\n",
    "    txt = re.sub(\"ى\", \"ي\", txt)\n",
    "    txt = re.sub(\"ؤ\", \"ء\", txt)\n",
    "    txt = re.sub(\"ئ\", \"ء\", txt)\n",
    "    txt = re.sub(\"ة\", \"ه\", txt)\n",
    "    txt = re.sub(\"گ\", \"ك\", txt)\n",
    "    print('remove longation', txt)\n",
    "    \n",
    "    # remove non-arabic words, or non-numbers, or non-english words in the text\n",
    "    txt = re.sub(r'[^a-zA-Z\\s0-9\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD.0-9]+'\n",
    "                 ,' ', txt)\n",
    "    print('remove non-arabic/non-english/non-number words', txt)\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b69e359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from farasa.stemmer import FarasaStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f332e8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ذهب\n"
     ]
    }
   ],
   "source": [
    "stemmer = FarasaStemmer()\n",
    "\n",
    "query_test = 'يذهب'\n",
    "stemmed_text = stemmer.stem(query_test)                                     \n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "55bb3a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords ،ذهب\n",
      "dediacritization ،ذهب\n",
      "Reduce Orthographic Ambiguity and Dialectal Variation ،ذهب\n",
      "Reduce Orthographic Ambiguity and Dialectal Variation ،ذهب\n",
      "Reduce Orthographic Ambiguity and Dialectal Variation ،ذهب\n",
      "Reducing Morphological Variation ، ذهب\n",
      "remove longation ، ذهب\n",
      "remove non-arabic/non-english/non-number words ، ذهب\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'، ذهب'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_test = '،ذهب'\n",
    "query_test_cleaned = text_clean2_steps(query_test)\n",
    "query_test_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc5760",
   "metadata": {},
   "source": [
    "## 5- Bulding Search Engine with Haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc039b7d",
   "metadata": {},
   "source": [
    "`Haystack` is an end-to-end framework that enables you to build powerful and production-ready pipelines for different search use cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23329062",
   "metadata": {},
   "source": [
    "We'll use haystack to build a scalable semantic search engine using the State-of-the-Art NLP models. built in a modular fashion so that you can combine the best technology from other open-source projects like Huggingface's Transformers, Elasticsearch, or Milvus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b7dc8",
   "metadata": {},
   "source": [
    "`there are 3 major components to Haystack.`\n",
    "\n",
    "- **Document Store**: Database storing the documents for our search. We recommend Elasticsearch, but have also more light-weight options for fast prototyping (SQL or In-Memory).\n",
    "\n",
    "- **Retriever**: Fast, simple algorithm that identifies candidate passages from a large collection of documents. Algorithms include TF-IDF or BM25, custom Elasticsearch queries, and embedding-based approaches. The Retriever helps to narrow down the scope for Reader to smaller units of text where a given question could be answered.\n",
    "\n",
    "- **Reader**: Powerful neural model that reads through texts in detail to find an answer. Use diverse models like BERT, RoBERTa or XLNet trained via FARM or Transformers on SQuAD like tasks. The Reader takes multiple passages of text as input and returns top-n answers with corresponding confidence scores. You can just load a pretrained model from Hugging Face's model hub or fine-tune it to your own domain data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9ba59ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import DocumentSearchPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1638c1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modaj\\OneDrive\\Documents\\Personal\\Jobs\\SHAI\\intern - task 3\\haystack-venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing necessary dependencies\n",
    "\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from haystack.utils.cleaning import clean_wiki_text\n",
    "from haystack.utils import convert_files_to_docs, fetch_archive_from_http  \n",
    "from haystack.nodes import FARMReader \n",
    "# from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14099f59",
   "metadata": {},
   "source": [
    "Haystack finds answers to queries within the documents stored in a `DocumentStore`. The current implementations of DocumentStore include `ElasticsearchDocumentStore`, `SQLDocumentStore`, and `InMemoryDocumentStore`.\n",
    "\n",
    "But they recommend ElasticsearchDocumentStore because as it comes preloaded with features like full-text queries, BM25 retrieval, and vector storage for text embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c1f16",
   "metadata": {},
   "source": [
    "`ElasticsearchDocumentStore` because as it **comes preloaded** with features like full-text queries, BM25 retrieval, and vector storage for text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "37fc8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "\n",
    "ELASTIC_PASSWORD = \"zJXerPHeN7PEmq5zWRuZ\"\n",
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", port=\"9200\", scheme='https',\n",
    "                                            username=\"elastic\", password=ELASTIC_PASSWORD, \n",
    "                                            index=\"se_shai_haystack\", ca_certs=\"../Certs/http_ca.crt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "18ab7f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all added documents (from previous runs)\n",
    "document_store.delete_all_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fb802a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dictionaries containing documents to our DB.\n",
    "document_store.write_documents(docs_df_cleaned2[['content_clean', 'title', 'url', 'tags', 'doc_id']]\n",
    "                               .rename(columns={'content_clean': 'content'}).to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7dcbd",
   "metadata": {},
   "source": [
    "`Retrievers` help narrowing down the scope for the Reader to smaller units of text where a given question could be answered. ( Elasticsearch's default BM25 algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "216b4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes.retriever.sparse import ElasticsearchRetriever\n",
    "from haystack.nodes import BM25Retriever\n",
    " \n",
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9532464",
   "metadata": {},
   "source": [
    "A `Reader` scans the texts returned by retrievers in detail and extracts the k best answers. They are based on powerful, but slower deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa47c8",
   "metadata": {},
   "source": [
    "`Haystack` currently supports Readers based on the frameworks `FARM` and `Transformers`. With both you can either load a local model or one from `Hugging Face's` model hub (https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "773e55a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium sized RoBERTa QA model using a Reader based on FARM (https://huggingface.co/deepset/roberta-base-squad2)\n",
    "# reader = FARMReader(model_name_or_path=\"aubmindlab/bert-base-arabertv2\", use_gpu=True, context_window_size=1000)\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2-covid\", use_gpu=True, context_window_size=1000)\n",
    "# reader = FARMReader(model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\", use_gpu=True, context_window_size=1000)\n",
    "\n",
    "# reader = FARMReader(model_name_or_path=\"ZeyadAhmed/AraElectra-Arabic-SQuADv2-QA\", use_gpu=True, context_window_size=1000)\n",
    "# reader = FARMReader(model_name_or_path=\"wissamantoun/araelectra-base-artydiqa\", use_gpu=True, context_window_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c256e",
   "metadata": {},
   "source": [
    "**FARMReader**: FARM makes Transfer Learning with BERT & Co simple, fast and enterprise-ready. It's built upon transformers and provides additional features to simplify the life of developers: Parallelized preprocessing, highly modular design, multi-task learning, experiment tracking, easy debugging and close integration with AWS SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd8a7c",
   "metadata": {},
   "source": [
    "With a `Haystack Pipeline` you can stick together your building blocks to a search pipeline. Under the hood, Pipelines are Directed Acyclic Graphs (DAGs) that you can easily customize for your own use cases. To speed things up, Haystack also comes with a few predefined Pipelines. One of them is the ExtractiveQAPipeline that combines a retriever and a reader to answer our questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c54682d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finder = Finder(reader, retriever)\n",
    "pipe = ExtractiveQAPipeline(reader, retriever)\n",
    "# pipe = DocumentSearchPipeline(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c985439a",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3b98e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_data4 = pd.read_csv('../Data/processed/SE_data4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "838cd569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: مولد نبوي\n",
      "\n",
      "\n",
      "#1\n",
      "Url: https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%A7%D9%84%D8%B9%D9%85%D9%84-%D8%B9%D8%B7%D9%84%D8%A9-%D8%A7%D9%84%D9%85%D9%88%D9%84%D8%AF-%D8%A7%D9%84%D9%86%D8%A8%D9%88%D9%8A\n",
      "Title: العمل: عطلة المولد النبوي الشريف تشمل القطاع الخاص\n",
      "Text: عمل   عطله مولد نبوي شريف شمل قطاع خاص اكد وزاره عمل ان عطل رسمي ماجور عامل مءسسه قطاع خاص اذا عمل استحق اجره اضافي واقع   150     جري معتاد وفق حكم ماده 59   ب قانون عمل اردني رقم   8   سنه 1996 تعديل . جاء توضيح ضوء بلاغ صادر رءيس وزير تعطيل وزاره داءره رسمي مءسسه هيءه عام احتفاء ذكري مولد نبوي شريف صادف سبت موافق ثامن اول ، مءكد ان بلاغ شمل قطاع خاص .\n",
      "\n",
      "\n",
      "\n",
      "#2\n",
      "Url: https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%A7%D9%84%D9%85%D9%88%D9%84%D8%AF-%D8%A7%D9%84%D9%86%D8%A8%D9%88%D9%8A-%D8%B9%D8%B7%D9%84%D8%A9-%D8%B1%D8%B3%D9%85%D9%8A%D8%A9\n",
      "Title: تعطيل الوزارات والدوائر الرسمية احتفاء بذكرى المولد النبوي\n",
      "Text: تعطيل وزاره داءره رسمي احتفاء ذكري مولد نبوي تقرر تعطيل وزاره داءره رسمي مءسسه هيءه عام جامعه رسمي بلديه مجلس خدمه مشترك امان عمان كبري شركه مملوك كامل حكومه ، ثلاثاء مقبل ، شهر ربيع اول 1443 هجري ، موافق تاسع شهر اول سنه 2021 ميلاد ، احتفاء ذكري مولد نبوي شريف . جاء قرار رءيس وزير دكتور شر الخصاونه ان بلاغ استثني وزاره داءره مءسسه رسمي اقتضي طبيعه عمل خلاف ذلك . اكد رءيس وزير وزاره داءره رسمي مءسسه هيءه عام اسهام ابراز مناسبه جليل ظهر ما لاق ب .\n",
      "\n",
      "\n",
      "\n",
      "#3\n",
      "Url: https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%B9%D8%B7%D9%84%D8%A9-%D8%B1%D8%B3%D9%85%D9%8A%D8%A9-%D8%A7%D9%84%D9%85%D9%88%D9%84%D8%AF-%D8%A7%D9%84%D9%86%D8%A8%D9%88%D9%8A\n",
      "Title: عطلة رسمية يوم السبت المقبل بذكرى المولد النبوي\n",
      "Text: عطله رسمي سبت ذكري مولد نبوي اصدر رءيس وزير دكتور شر الخصاونه بلاغ تعطيل دوام وزاره داءره رسمي ، مءسسه هيءه عام ، جامعه رسمي ، بلديه مجلس خدمه مشترك امان عمان شركه مملوك كامل حكومه ؛ سبت احتفاء ذكري مولد نبوي شريف صادف شهر ربيع اول سنه 1444 هجري ، موافق 8 اول سنه 2022 م . استثني بلاغ داءره رسمي وزاره مءسسه اقتضي طبيعه عمل خلاف ذلك . دعا رءيس وزير احياء هذا ذكري عطر وزاره داءره رسمي مءسسه هيءه عام اسهام ابراز مناسبه جليل ظهر ما لاق ب .\n",
      "\n",
      "\n",
      "\n",
      "#4\n",
      "Url: https://husna.fm/%D8%AD%D8%B3%D9%86%D9%89/%D9%85%D8%AD%D9%84%D9%8A-%D8%AD%D8%B2%D8%A8-%D8%AC%D8%A8%D9%87%D8%A9-%D8%A7%D9%84%D8%B9%D9%85%D9%84-%D8%A7%D9%84%D8%A5%D8%B3%D9%84%D8%A7%D9%85%D9%8A\n",
      "Title: السياحة ترفض طلب حزب الجبهة إقامة حفل المولد النبوي\n",
      "Text: سياحه رفض طلب حزب جبهه اقام حفل مولد نبوي تقدم حزب جبهه عمل اسلامي يوم طلب الي وزاره سياحه اقام حفل مناسبه مولد نبوي مدرج روماني الا ان وزاره رفض سماح ابداء سبب . اكد ناطق حزب ثابت عساف ل حسني اربعاء ان وزاره امتنع اعطاء كتاب خطي رفض اكتفي رد شفهي يوم تقديم حزب طلب . عساف ان حزب تقدم طلب وزاره سياحه قام جهه باستمزاج راي محافظ عاصمه ، اتي رد عقب رفض . اعتبر حزب بيان   ان رفض خطوه مريب كشف نيه صانع قرار منع فضيله محاربه سماح اقامه حفله مجون رذيله اخير جرش غير     وصف بيان   . عبر حزب قلق بالغ تصرف مطالب حكومه تعامل عداله ما تعلق تطبيق امر دفاع عدم استخدام اداه تضييق منع خارج دور مواجهه وباء .\n",
      "\n",
      "\n",
      "\n",
      "#5\n",
      "Url: https://husna.fm/%D8%B9%D8%B1%D8%A8%D9%8A-%D9%88-%D8%AF%D9%88%D9%84%D9%8A/%D9%85%D8%A4%D8%AA%D9%85%D8%B1-%D8%AF%D9%88%D9%84%D9%8A%C2%A0%D8%AD%D9%88%D9%84-%D8%A7%D9%84%D9%82%D9%8A%D9%85-%D8%A7%D9%84%D8%AD%D8%B6%D8%A7%D8%B1%D9%8A%D8%A9-%D8%A8%D8%A7%D9%84%D8%B3%D9%8A%D8%B1%D8%A9-%D8%A7%D9%84%D9%86%D8%A8%D9%88%D9%8A%D8%A9-%D9%81%D9%8A-27-%D8%A3%D9%8A%D8%A7%D8%B1-%D8%A8%D8%A7%D9%84%D9%85%D8%BA%D8%B1%D8%A8\n",
      "Title: مؤتمر دولي حول القيم الحضارية بالسيرة النبوية في 27 أيار بالمغرب\n",
      "Text: مءتمر دولي قيمه حضاري سيره نبوي 27 ايار مغرب اعلن منظمه عالم اسلامي تربيه علم ثقافه   ايسيسكو   ان مءتمر دولي قيمه حضاري سيره نبوي ، عقد شعار   نحو رءيه مستقبل سيره نبوي   ، 27 ايار   مايو   جاري رباط مغرب . مدير عام للايسيسكو دكتور سالم محمد مالك ، ان مءتمر عقد منظمه شراكه رابطه محمدي عالم مملكه مغربي رابطه عالم اسلامي اتي رد علمي عملي محاوله اساءه الي مصطفي صلي الله سلم ، اتي اطار رءيه الايسيسكو استراتيجي عمل جديد ، جعل منار اشعاع حضاري دولي ، عمل تقديم صوره حقيقي حضاره ثقافه اسلامي ، ترسيخ قيمه تعايش حوار جميع ، تحقيق سلم بناء مستقبل مشرق عالم . اشار مدير عام للايسيسكو الي ان مءتمر هدف الي اعداد استراتيجي واصل تعاطي بعد وظيفي سيره نبوي ، حصر اشكال ذاتي موضوعي تعامل مع ، تجميع ترتيب جهد علمي مجال بحث سيره نبوي ، بناء رءيه متجدد افق مستقبل ل ، ذلك تمكين منظمه هيءه عامل مجال تربيه ثقافه علم اتصال الي اداه عمل منهج بناء مخطط عمل خبره مجال تعريف سيره نبوي . اوضح ان مءتمر تناول اربع محور رءيس ، هي   سيره نبوي جهد علمي ، سيره نبوي بعد وظيفي ، سيره نبوي اشكال معاصر ، شهاده عالم حي سيد انام رساله خاتمه ، اصدر مءتمر اعلان توصيه تنفيذي تم تعميم عدد لغه منظمه دولي مءسسه هيءه مختص ثقافه حوار حضاري .\n",
      "\n",
      "\n",
      "\n",
      "#6\n",
      "Url: https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%B0%D9%83%D8%B1%D9%89-%D8%A7%D9%84%D9%87%D8%AC%D8%B1%D8%A9-%D8%A7%D9%84%D9%86%D8%A8%D9%88%D9%8A%D8%A9-%D8%AF%D8%B1%D9%88%D8%B3-%D9%86%D8%B3%D8%AA%D9%84%D9%87%D9%85-%D9%85%D9%86%D9%87%D8%A7-%D8%A7%D9%84%D8%B9%D8%A8%D8%B1-%D9%81%D9%8A-%D8%A8%D9%86%D8%A7%D8%A1-%D8%A7%D9%84%D8%AF%D9%88%D9%84%D8%A9\n",
      "Title: ذكرى الهجرة النبوية دروس نستلهم منها العبر في بناء الدولة\n",
      "Text: ذكري هجره نبوي درس استلهم عبره بناء دوله اتي ذكري هجره نبوي شريف صادف ثلاث تموز 2022 موافق اول محرم سنه 1444 هجري ؛ اعطي درس عبره صبر تكاتف بناء دوله تثبيت ركن اقتصاد قانون . احيي هجره نبوي عديد معني سامي ، مفتاح بناء دوله ، غير مجري تاريخ نهج اسلامي نشر اسلام ، اهم ميز مءاخاه مهاجر هاجر دار استجابه نداء الهي رباني ، رسول كريم مثال قدوه حمل حمل اجل رساله حق اخراج ناس ظلم ه الي نور انتقل مكه مدينه مهاجر ، استقبل انصار ، اساس اول مجتمع اسلامي تاسس دوله حقيقي   ارض مدينه منور جمع هجره شعب مختلف اعراق لغه وحد بين صنع من ام . ذكر استاذ فقه اسلامي دكتور انس ابو عطا ان هجره نبوي شريف علم حسن تدبير اخذ سبب ، بناء دوله قانون ، الله عز جل حين اراد تكريم نبي صلي الله سلم اسر ب راق ، لكن اراد نبي صلي الله سلم ان قدوه اسوه معلم ناس ، اذن هجره نبوي شريف . ابو عطا حديث الي حسني ان هجره عنوان رءيس اخذ سبب ، ان اسلام دين بذل عطاء تضحي ، مكث نبي صلي الله سلم عاد خط هجره 4 شهر كامل ، اخذ سبب تفصيل تفصيل ، هذا اول محطه هجره نبوي شريف . اما محطه ثاني ، شارك ف رد مجتمع خط هجره ، اختار نبي صلي الله سلم ابو بكر صديق صاحب صديق رحله هجره دلاله حسن اختيار زواج ، حسن اختيار جار ، حسن اختيار عمل ، عند هاجر رسول حمل مشقه صعب اجل نيل رضا الله ، صحبه صالح اختار سفر اعان خير . شارك مرا ه ، غلام ، صبي ، شارك ايض كافر ، هنا اشار الي اختيار شخص صاحب كفاءات قيام مهمه ، غض نظر دين او عمر . اما محطه ثالث هي ضروره بذل مال جهد اجل دين ، عكس ، بذل دين اجل مال ، تبرع ابو بكر صديق ناقه هجره ، ما بني نبي صلي الله عليم سلم مسجد قباء ايض بذل مال شراء يتيم ، هجره ليس امر سهل ، هاجر اذا استطاع تغيير منكر بلد عاش في ، اذا استطاع علي سعي دوم انكار منكر كل طريق ممكن .\n",
      "\n",
      "\n",
      "\n",
      "#7\n",
      "Url: https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%A7%D9%84%D8%B3%D8%B9%D9%88%D8%AF%D9%8A%D8%A9-%D8%AA%D9%81%D8%AA%D8%AD-%D8%A8%D8%A7%D8%A8-%D8%A7%D9%84%D8%B9%D9%85%D8%B1%D8%A9-%D8%A8%D8%AF%D8%A1%D8%A7%D9%8B-%D9%85%D9%86-%D8%A7%D9%84%D9%8A%D9%88%D9%85\n",
      "Title: السعودية تفتح باب العمرة بدءاً من اليوم\n",
      "Text: سعودي فتح باب عمر ه بدء اعلن وزاره حج عمر ه ، بدء استقبال طلب اصدار تاشير ضيف رحمن قادم خارج مملكه دوله عالم ، تاديه منسك عمر ه زياره مسجد نبوي شريف ، موسم عمر ه عام 1444ه ، اعتبار خميس خامس شهر حجه جاري عام 1443ه ، موافق 14   7   2022م . اوضح وزاره بيان صحيفه امس اربعاء ، ان بدء عمر ه للمعتمرين داخل خارج مملكه بدا اغر شهر محرم عام 1444ه ، موافق 30   7   2022م . اصدار تصريح للمعتمرين داخل مملكه تطبيق   اعتمر   ، وسط منظومه متكامل خدمه تدبير صحي معتمد جهه معني حفاظ سلامه صحه المعتمرين زاءر ، اجراء سهل تضمن اداء منسك عمر ه طمانينه سار . شمل ضابط تاهيل وكيل خارجي تقديم خدمه المعتمرين زاءر مسجد نبوي شريف ، تقديم طلب تاهيل عبر بوابه وزاره حج عمر ه ، التزام ضابط موضح بوابه ، رفقه مستندات بيان متعلق سجل تجاري ، عضو منظمه دولي نقل جوي   الاياتا   ، عدد بيان شخصيه اخري . ذكر ان وزاره حج عمر ه عمل تنسيق جهه مختص تحديد اجراء صحي وقاءي ، حمايه فيروس   كوفيد 19   ، من   حصول اللقاحات معتمد مملكه ، رفقه شهاده تحصين المصادق جهه رسمي بلد المعتمر ، اضاف الي اقرار صحه معلومه .\n",
      "\n",
      "\n",
      "\n",
      "#8\n",
      "Url: https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%A7%D9%84%D8%B5%D8%AD%D8%A9-%D8%AA%D8%AE%D9%81%D9%8A%D8%B6-%D8%A3%D8%B3%D8%B9%D8%A7%D8%B1-%D9%81%D8%AD%D8%B5%D9%8A-%D8%A7%D9%84%D8%A3%D8%AC%D8%B3%D8%A7%D9%85-%D8%A7%D9%84%D9%85%D8%B6%D8%A7%D8%AF%D8%A9-%D9%88%D9%85%D9%88%D9%84%D8%AF%D8%A7%D8%AA-%D8%A7%D9%84%D8%B6%D8%AF-%D8%A7%D9%84%D8%B3%D8%B1%D9%8A%D8%B9\n",
      "Title: الصحة : تخفيض أسعار فحصي الأجسام المضادة ومولدات الضد السريع\n",
      "Text: صحه   تخفيض سعر فحص جسم مضاد مولد ضد سريع قرر وزير صحه دكتور نذير عبيدات ، تخفيض سعر فحص جسم مضاد مولد ضد سريع ، وفق ذكر رمدير مدير ترخيص مهنه مءسسه دكتور ناصر الخشمان ، اشار الخشمان تحديد سعر فحص جسم مضاد ب10 دينار كل فحص ، فحص مولد ضد سريع ب7 دينار ، لافت ان سعر جديد اعتبر و ري بدء   ثلاثاء   . جاء تعديل سعر الفحصين بناء توصيه وزير صحه لجنه مختبر طبي خاص .\n",
      "\n",
      "\n",
      "\n",
      "#9\n",
      "Url: https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D9%85%D8%B3%D8%AA%D8%B4%D9%81%D9%89-%D8%A5%D8%B1%D8%A8%D8%AF-%D8%A7%D9%84%D9%85%D9%8A%D8%AF%D8%A7%D9%86%D9%8A-%D8%B6%D9%85%D9%86-%D8%AF%D8%A7%D8%A6%D8%B1%D8%A9-%D9%82%D8%B7%D8%B9-%D8%A7%D9%84%D9%83%D9%87%D8%B1%D8%A8%D8%A7%D8%A1-%D8%BA%D8%AF%D8%A7\n",
      "Title: مستشفى إربد الميداني ضمن دائرة قطع الكهرباء غدا\n",
      "Text: مستشفي اربد ميداني داءره قطع كهرباء وجه شركه كهرباء اربد كتاب مستشفي اربد ميداني علم نيه اجراء قطع مبرمج كهرباء مده 6 ساعه ، صيانه شبكه . دور خاطب اداره مستشفي ميداني شركه فور اتخاذ اجراء اما تاجيل قطعه او تزويد مستشفي مولد اضافي ، د . عماد ابو يقين ان قطع كهرباء   اسوا كابوس   ان حصل ، اعتماد مريض جهاز تنفس بينما مدير كهرباء اربد بشار تميمي ل   حسني ان فور مخاطبه مستشفي ارسل فريق معاينه موقع دراسه وضع مولد متنقل ، تغذيه مستشفي طويل راوند قطعه . ذكر ان مستشفي ميداني اربد مزود اصل مولد كهرباءي تشغيل حال قطع تيار كهرباءي شركه كهرباء وطني ، استطاع مولد تشغيل مستشفي كامل وحده تجهيز اضاف الي جهاز تكييف خاص قسم عنايه حثيث اتسع ل50 سرير . علم حسني اداره مستشفي ان مريض مستشفي ان يتجاوز30 مريض ، ممكن استيعاب جميع قسم عنايه حثيث ضمان عنايه ب اكمل وجه حال قطع تيار مصدر .\n",
      "\n",
      "\n",
      "\n",
      "#10\n",
      "Url: https://husna.fm/%D8%B9%D8%B1%D8%A8%D9%8A-%D9%88-%D8%AF%D9%88%D9%84%D9%8A/%D8%A7%D9%84%D8%AD%D8%B1%D9%85%D9%8A%D9%86-%D8%A7%D9%84%D8%B4%D8%B1%D9%8A%D9%81%D9%8A%D9%86-%D9%84%D8%B5%D9%84%D8%A7%D8%A9\n",
      "Title: استووا..اعتدلوا..تراصوا..في الحرمين الشريفين تعود إلى سيرتها الأولى\n",
      "Text: استوي . . اعتدل . . تراص . . في حرم شريف عاد الي سيره اول مشهد مهيب اد مصلي فجر احد صلاه فجر حرم شريف مكه مكرم مدينه منور تباعد ، ذلك اعلان سلطه سعودي تخفيف اشتراط صحي لازم فرض مواجهه جاءح كور . نشر اماره منطقه مكه مكرم تغريد تويتر اظهر لحظه اصطفاف مصلي صلاه فجر هم متراصون .   فيديو لحظه اصطفاف مصلي ل   صلاه   فجر   مسجد   حرام 11 ربيع اول 1443ه https       t . co   keSyOIWlV7 pic . twitter . com   ePM1XGw6jw اظهر فيديو امام مسجد نبوي شريف مدينه منور التفت الي مصلي قاءل     استوي . . سوي صف . . سد خلل   فيديو   امام   مسجد   نبوي التفت قاءل     استوا . . سوي صف . . سد فرج   مشهد   صلاه   فجر   مسجد   نبوي تخفيف اجراء الاحترازيه   اخباري pic . twitter . com   ZVdruJ8lgD مساء سبت ، عمل اداره حرم مكي نزع ملصق تباعد جسد ي ملصق ارض ، ذلك قبيل استقبال حرم مكي مصلي طاقه استيعاب ي كامل . انتشر عبر تويتر تغريد اظهر عامل هم نزع ملصق وسط ارتفاع صوت تهليل حمد فرح عوده حرم مكي شريف الي حاله طبيعي استقبال مصلي طاقه استيعاب ي كامل .   مسجد   حرام الحمدلله بلغ حمد منتهي الحمدلله اعاد ل حياه جديد اللهم احفظ احب ابعد عن عن مسلم وباء مرض         pic . twitter . com   RtN0Fv7qzN قال رءاسه شان حرم عبر تويتر   بعد نجاح اجراء الاحترازيه لفايروس كور ، انهاء خدمه ملصق تباعد جسد ي عام مسجد حرام   . نجاح اجراء الاحترازيه لفايروس كور انهاء خدمه ملصق تباعد جسد ي عام مسجد حرام . https       t . co   sOqFVV4ZQ5   رءاسه   شان   حرم pic . twitter . com   7OIAwTUWCi اسبوع ماضي ، اعلن مملكه عربي سعودي عود حرم مكي مدني احد استقبال جموع مصلي كامل طاقه استيعاب ي ، الغاء تباعد جسد ي مصلي ، ابقاء ارتداء كمامه مكان مغلق حرم شريف اضاف الي شرط حصول جرعه لقاح مضاد فيروس كور . اداره شان حرم ان تخفيف اجراء الاحترازيه حرم اتي   تحقيق تقدم عمليه تحصين مجتمع انخفاض اصابه   . The presidency of Haramain has prepared staff at Masjid Al Haram to keep pace with the decision to ease major restrictions   in an effort to provide high   quality service to worshippers .   haramaininfo pic . twitter . com   RkAEawSUmN سبب جاءح كوفيد   19 اجري سعودي موسمي حج ماضي محدود ، مقتصر مقيم داخل ارض مملكه عدد 10 الف رفع الي 60 الف تعليق موسم عمر ه عده شهر ، استءناف 4 مرحله بدا اول   اكتوبر 2020 . اب   اغسطس 2021 رفعت سعودي المعتمرين الي 2 معتمر شهري 600 الف .\n",
      "\n",
      "\n",
      "\n",
      "search duration:  0.004997730255126953 ms\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "q = \"مولد نبوي\"\n",
    "q_processed = text_clean2(q)\n",
    "number_of_answers_to_fetch = 5\n",
    "print()\n",
    "\n",
    "# prediction = finder.get_answers(question=question, top_k_retriever=20, top_k_reader=number_of_answers_to_fetch)\n",
    "prediction = pipe.run(\n",
    "    query=q_processed, params=\n",
    "    {\"Retriever\": {\"top_k\": 10}}\n",
    ")\n",
    "\n",
    "print(f\"Query: {prediction['query']}\")\n",
    "print(\"\\n\")\n",
    "for i in range(len(prediction['documents'])):\n",
    "    print(f\"#{i+1}\")\n",
    "#     print(prediction)\n",
    "#     print(f\"Answer: {prediction['answers'][i]['answer']}\")\n",
    "    print(f\"Url: {prediction['documents'][i].meta['url']}\")    # from data stored in db\n",
    "    print(f\"Title: {prediction['documents'][i].meta['title']}\")  # from data stored in db\n",
    "    print(f\"Text: {prediction['documents'][i].content}\")    # from data stored in db\n",
    "#     print(f\"Context: {prediction['answers'][i]['context']}\")  # a 1000 words context around the answer\n",
    "    print('\\n\\n')\n",
    "    \n",
    "print('search duration: ', time.time() - start_time, 'ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0b341cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.56 Batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: مولد نبوي\n",
      "\n",
      "\n",
      "search duration:  1.3164136409759521 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "q = \"مولد نبوي\"\n",
    "q_processed = text_clean2(q)\n",
    "number_of_answers_to_fetch = 5\n",
    "print()\n",
    "\n",
    "# prediction = finder.get_answers(question=question, top_k_retriever=20, top_k_reader=number_of_answers_to_fetch)\n",
    "prediction = pipe.run(\n",
    "    query=q_processed, params=\n",
    "    {\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": number_of_answers_to_fetch}}\n",
    ")\n",
    "\n",
    "# prediction['documents'][0].meta\n",
    "# prediction['answers']\n",
    "print(f\"Query: {prediction['query']}\")\n",
    "print(\"\\n\")\n",
    "# for i in range(len(prediction)):\n",
    "#     print(f\"#{i+1}\")\n",
    "# #     print(prediction)\n",
    "# #     print(f\"Answer: {prediction['answers'][i]['answer']}\")\n",
    "#     print(f\"Url: {prediction['documents'][i].meta['url']}\")    # from data stored in db\n",
    "#     print(f\"Title: {prediction['documents'][i].meta['title']}\")  # from data stored in db\n",
    "#     print(f\"Text: {prediction['documents'][i].content}\")    # from data stored in db\n",
    "# #     print(f\"Context: {prediction['answers'][i]['context']}\")  # a 1000 words context around the answer\n",
    "#     print('\\n\\n')\n",
    "    \n",
    "print('search duration: ', time.time() - start_time, 'ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5aca682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bbe37d7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Answer {'answer': 'ي', 'type': 'extractive', 'score': 0.2571229934692383, 'context': 'تعطيل وزاره داءره رسمي احتفاء ذكري مولد نبوي تقرر تعطيل وزاره داءره رسمي مءسسه هيءه عام جامعه رسمي بلديه مجلس خدمه مشترك امان عمان كبري شركه مملوك كامل حكومه ، ثلاثاء مقبل ، شهر ربيع اول 1443 هجري ، موافق تاسع شهر اول سنه 2021 ميلاد ، احتفاء ذكري مولد نبوي شريف . جاء قرار رءيس وزير دكتور شر الخصاونه ان بلاغ استثني وزاره داءره مءسسه رسمي اقتضي طبيعه عمل خلاف ذلك . اكد رءيس وزير وزاره داءره رسمي مءسسه هيءه عام اسهام ابراز مناسبه جليل ظهر ما لاق ب .', 'offsets_in_document': [{'start': 245, 'end': 246}], 'offsets_in_context': [{'start': 245, 'end': 246}], 'document_id': '90ed7478671f3438e4ed0047912f501c', 'meta': {'title': 'تعطيل الوزارات والدوائر الرسمية احتفاء بذكرى المولد النبوي', 'url': 'https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%A7%D9%84%D9%85%D9%88%D9%84%D8%AF-%D8%A7%D9%84%D9%86%D8%A8%D9%88%D9%8A-%D8%B9%D8%B7%D9%84%D8%A9-%D8%B1%D8%B3%D9%85%D9%8A%D8%A9', 'tags': ['المولد النبوي', 'النبي محمد صلّى الله عليه وسلم'], 'doc_id': 571}}>,\n",
       " <Answer {'answer': 'ي شريف صادف شهر ربيع اول', 'type': 'extractive', 'score': 0.15067413449287415, 'context': 'عطله رسمي سبت ذكري مولد نبوي اصدر رءيس وزير دكتور شر الخصاونه بلاغ تعطيل دوام وزاره داءره رسمي ، مءسسه هيءه عام ، جامعه رسمي ، بلديه مجلس خدمه مشترك امان عمان شركه مملوك كامل حكومه ؛ سبت احتفاء ذكري مولد نبوي شريف صادف شهر ربيع اول سنه 1444 هجري ، موافق 8 اول سنه 2022 م . استثني بلاغ داءره رسمي وزاره مءسسه اقتضي طبيعه عمل خلاف ذلك . دعا رءيس وزير احياء هذا ذكري عطر وزاره داءره رسمي مءسسه هيءه عام اسهام ابراز مناسبه جليل ظهر ما لاق ب .', 'offsets_in_document': [{'start': 207, 'end': 231}], 'offsets_in_context': [{'start': 207, 'end': 231}], 'document_id': 'b96cdafc0db66790bbdd9e1181dcce9b', 'meta': {'title': 'عطلة رسمية يوم السبت المقبل بذكرى المولد النبوي', 'url': 'https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%B9%D8%B7%D9%84%D8%A9-%D8%B1%D8%B3%D9%85%D9%8A%D8%A9-%D8%A7%D9%84%D9%85%D9%88%D9%84%D8%AF-%D8%A7%D9%84%D9%86%D8%A8%D9%88%D9%8A', 'tags': ['عطلة رسمية', 'المولد النبوي'], 'doc_id': 578}}>,\n",
       " <Answer {'answer': 'ي مدرج رومان', 'type': 'extractive', 'score': 0.15060532093048096, 'context': 'سياحه رفض طلب حزب جبهه اقام حفل مولد نبوي تقدم حزب جبهه عمل اسلامي يوم طلب الي وزاره سياحه اقام حفل مناسبه مولد نبوي مدرج روماني الا ان وزاره رفض سماح ابداء سبب . اكد ناطق حزب ثابت عساف ل حسني اربعاء ان وزاره امتنع اعطاء كتاب خطي رفض اكتفي رد شفهي يوم تقديم حزب طلب . عساف ان حزب تقدم طلب وزاره سياحه قام جهه باستمزاج راي محافظ عاصمه ، اتي رد عقب رفض . اعتبر حزب بيان   ان رفض خطوه مريب كشف نيه صانع قرار منع فضيله محاربه سماح اقامه حفله مجون رذيله اخير جرش غير     وصف بيان   . عبر حزب قلق بالغ تصرف مطالب حكومه تعامل عداله ما تعلق تطبيق امر دفاع عدم استخدام اداه تضييق منع خارج دور مواجهه وباء .', 'offsets_in_document': [{'start': 115, 'end': 127}], 'offsets_in_context': [{'start': 115, 'end': 127}], 'document_id': 'a64da2a3e87352cfb84c91dc150a630e', 'meta': {'title': 'السياحة ترفض طلب حزب الجبهة إقامة حفل المولد النبوي', 'url': 'https://husna.fm/%D8%AD%D8%B3%D9%86%D9%89/%D9%85%D8%AD%D9%84%D9%8A-%D8%AD%D8%B2%D8%A8-%D8%AC%D8%A8%D9%87%D8%A9-%D8%A7%D9%84%D8%B9%D9%85%D9%84-%D8%A7%D9%84%D8%A5%D8%B3%D9%84%D8%A7%D9%85%D9%8A', 'tags': ['حزب جبهة العمل الإسلامي', 'السياحة والآثار'], 'doc_id': 357}}>,\n",
       " <Answer {'answer': 'ي مولد نبوي شريف صادف شهر ربيع اول', 'type': 'extractive', 'score': 0.1269761472940445, 'context': 'عطله رسمي سبت ذكري مولد نبوي اصدر رءيس وزير دكتور شر الخصاونه بلاغ تعطيل دوام وزاره داءره رسمي ، مءسسه هيءه عام ، جامعه رسمي ، بلديه مجلس خدمه مشترك امان عمان شركه مملوك كامل حكومه ؛ سبت احتفاء ذكري مولد نبوي شريف صادف شهر ربيع اول سنه 1444 هجري ، موافق 8 اول سنه 2022 م . استثني بلاغ داءره رسمي وزاره مءسسه اقتضي طبيعه عمل خلاف ذلك . دعا رءيس وزير احياء هذا ذكري عطر وزاره داءره رسمي مءسسه هيءه عام اسهام ابراز مناسبه جليل ظهر ما لاق ب .', 'offsets_in_document': [{'start': 197, 'end': 231}], 'offsets_in_context': [{'start': 197, 'end': 231}], 'document_id': 'b96cdafc0db66790bbdd9e1181dcce9b', 'meta': {'title': 'عطلة رسمية يوم السبت المقبل بذكرى المولد النبوي', 'url': 'https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%B9%D8%B7%D9%84%D8%A9-%D8%B1%D8%B3%D9%85%D9%8A%D8%A9-%D8%A7%D9%84%D9%85%D9%88%D9%84%D8%AF-%D8%A7%D9%84%D9%86%D8%A8%D9%88%D9%8A', 'tags': ['عطلة رسمية', 'المولد النبوي'], 'doc_id': 578}}>,\n",
       " <Answer {'answer': 'ه', 'type': 'extractive', 'score': 0.10337495058774948, 'context': 'ولي قيمه حضاري سيره نبوي ، عقد شعار   نحو رءيه مستقبل سيره نبوي   ، 27 ايار   مايو   جاري رباط مغرب . مدير عام للايسيسكو دكتور سالم محمد مالك ، ان مءتمر عقد منظمه شراكه رابطه محمدي عالم مملكه مغربي رابطه عالم اسلامي اتي رد علمي عملي محاوله اساءه الي مصطفي صلي الله سلم ، اتي اطار رءيه الايسيسكو استراتيجي عمل جديد ، جعل منار اشعاع حضاري دولي ، عمل تقديم صوره حقيقي حضاره ثقافه اسلامي ، ترسيخ قيمه تعايش حوار جميع ، تحقيق سلم بناء مستقبل مشرق عالم . اشار مدير عام للايسيسكو الي ان مءتمر هدف الي اعداد استراتيجي واصل تعاطي بعد وظيفي سيره نبوي ، حصر اشكال ذاتي موضوعي تعامل مع ، تجميع ترتيب جهد علمي مجال بحث سيره نبوي ، بناء رءيه متجدد افق مستقبل ل ، ذلك تمكين منظمه هيءه عامل مجال تربيه ثقافه علم اتصال الي اداه عمل منهج بناء مخطط عمل خبره مجال تعريف سيره نبوي . اوضح ان مءتمر تناول اربع محور رءيس ، هي   سيره نبوي جهد علمي ، سيره نبوي بعد وظيفي ، سيره نبوي اشكال معاصر ، شهاده عالم حي سيد انام رساله خاتمه ، اصدر مءتمر اعلان توصيه تنفيذي تم تعميم عدد لغه منظمه دولي مءسسه هيءه مختص ثقافه حوار حضاري .', 'offsets_in_document': [{'start': 715, 'end': 716}], 'offsets_in_context': [{'start': 609, 'end': 610}], 'document_id': 'cc72c85ca4c425f23ddfdc154bfa9133', 'meta': {'title': 'مؤتمر دولي\\xa0حول القيم الحضارية بالسيرة النبوية في 27 أيار بالمغرب', 'url': 'https://husna.fm/%D8%B9%D8%B1%D8%A8%D9%8A-%D9%88-%D8%AF%D9%88%D9%84%D9%8A/%D9%85%D8%A4%D8%AA%D9%85%D8%B1-%D8%AF%D9%88%D9%84%D9%8A%C2%A0%D8%AD%D9%88%D9%84-%D8%A7%D9%84%D9%82%D9%8A%D9%85-%D8%A7%D9%84%D8%AD%D8%B6%D8%A7%D8%B1%D9%8A%D8%A9-%D8%A8%D8%A7%D9%84%D8%B3%D9%8A%D8%B1%D8%A9-%D8%A7%D9%84%D9%86%D8%A8%D9%88%D9%8A%D8%A9-%D9%81%D9%8A-27-%D8%A3%D9%8A%D8%A7%D8%B1-%D8%A8%D8%A7%D9%84%D9%85%D8%BA%D8%B1%D8%A8', 'tags': ['مؤتمر دولي\\xa0'], 'doc_id': 3903}}>]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction['answers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6db10784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # different approach\n",
    "# from haystack.nodes import EmbeddingRetriever\n",
    "# from haystack.pipelines import FAQPipeline #initialize a pipeline and ask questions\n",
    "\n",
    "# ELASTIC_PASSWORD = \"zJXerPHeN7PEmq5zWRuZ\"\n",
    "# document_store = ElasticsearchDocumentStore(host=\"localhost\", port=\"9200\", scheme='https',\n",
    "#                                             username=\"elastic\", password=ELASTIC_PASSWORD, \n",
    "#                                             index=\"se_shai_haystack2\", ca_certs=\"../Certs/http_ca.crt\",\n",
    "#                                             embedding_dim=384, embedding_field=\"content_emb\",  \n",
    "# #                                             excluded_meta_data=[\"content\"],\n",
    "#                                             similarity='cosine' )\n",
    "# document_store.delete_all_documents()\n",
    "\n",
    "# retriever = EmbeddingRetriever( \n",
    "#     document_store=document_store, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\", \n",
    "#     use_gpu=True , model_format='sentence_transformers'\n",
    "#     #instead of retrieving via Elasticsearch's plain BM25, we want to use vector similarity of \n",
    "#     #the questions (user question vs. FAQ ones).\n",
    "# )\n",
    "# document_store.write_documents(docs_df_cleaned2[['content_clean', 'title', 'url', 'tags', 'doc_id']]\n",
    "#                                .rename(columns={'content_clean': 'content'}).to_dict(orient='records'))\n",
    "# document_store.update_embeddings(retriever=retriever)\n",
    "\n",
    "# pipe = FAQPipeline(retriever=retriever) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# q = \"نبي\"\n",
    "# q_processed = text_clean2(q)\n",
    "# number_of_answers_to_fetch = 10\n",
    "# print(q_processed)\n",
    "\n",
    "# # prediction = finder.get_answers(question=question, top_k_retriever=20, top_k_reader=number_of_answers_to_fetch)\n",
    "# # prediction = pipe.run(\n",
    "# #     query=q_processed, params=\n",
    "# #     {\"Retriever\": {\"top_k\": 10}}\n",
    "# # )\n",
    "# prediction = retriever.retrieve(q_processed, top_k = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack-venv",
   "language": "python",
   "name": "haystack-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
