{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa97ceac",
   "metadata": {},
   "source": [
    "- a software system to bring out information of web\n",
    "\n",
    "- smart search engines can be considered unsupervised learning approaches, due to the nature of clustering related information without such label in hand\n",
    "\n",
    "- Search Engines have evolved **from a text input and output service to** an experience that cuts across voice, video, documents, and conversations\n",
    "\n",
    "\n",
    "- an **infinite problem** to solve\n",
    "\n",
    "\n",
    "- **related** to information retrieval, language understanding\n",
    "\n",
    "\n",
    "- the **value that an effective search tool can bring to a business is enormous**; a key piece of intellectual property. Often a search bar is the main interface between customers and the business. \n",
    "    - create a competitive advantage by delivering an improved user experience.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1a2c1f",
   "metadata": {},
   "source": [
    "search engine popular approaches:\n",
    "- manual implementation with dataframe + tf-idf\n",
    "- Elastic Search + BM25\n",
    "- BM25 + Azure Cognitive Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40924fed",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "- Search index for storing each document, reflecting relevant information and up to date information\n",
    "    - data can be reorganized by date (suggestion)\n",
    "- Query understanding\n",
    "    - takes sentence and preprocessed data information **directly without much context**\n",
    "    - we can extract words or tokens from the query to match **article_type** (suggestion)\n",
    "        - query to match tags (done)\n",
    "    - we can filter the search by either blog or News (suggestion)\n",
    "        - or add multiple results available (blog, News, or both)\n",
    "    - BM25 + Azure Cognitive search\n",
    "- Query ranking\n",
    "    - by consine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f41a6",
   "metadata": {},
   "source": [
    "## 1- Library and Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd3ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# for text cleaning and preprocessing\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2ec345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.read_json('../Data/husna.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f9280",
   "metadata": {},
   "source": [
    "## 2- Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3dccb",
   "metadata": {},
   "source": [
    "#### 2.1 preparing data for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5990710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "docs_df = docs_df.drop(columns=['publisher', 'crawled_at', 'published_at'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6134b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df_dropped = docs_df.drop(index=\n",
    "                               docs_df[(docs_df['content'].str.len() == 0) & (docs_df['title'] == '')].index, axis=0)\n",
    "docs_df_dropped = docs_df_dropped.reset_index(drop=True)\n",
    "docs_df = docs_df_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f3d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df['text'] = docs_df['content'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81137f2",
   "metadata": {},
   "source": [
    "## 3- Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c326af",
   "metadata": {},
   "source": [
    "important data cleaning functions:\n",
    "- remove punctuation\n",
    "- tokenization \n",
    "- stem words\n",
    "\n",
    "**cleaning functions not implemented**: removing repeating characters, stop words, emoji, hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f88040c",
   "metadata": {},
   "source": [
    "### data cleaning (ver.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adfcff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df_cleaned2 = docs_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31a52569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dediacritization tool\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "# Reducing Orthographic Ambiguity\n",
    "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_ar\n",
    "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
    "\n",
    "# toknenization\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "\n",
    "# Morphological Disambiguation (Maximum Likelihood Disambiguator)\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "mle = MLEDisambiguator.pretrained() # instantiation fo MLE disambiguator\n",
    "\n",
    "# tokenization / lemmatization (choosing approach that best fit the project)\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbe28255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "def remove_urls(text):\n",
    "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ', text)\n",
    "\n",
    "# 5\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").text\n",
    "\n",
    "# removing symbols\n",
    "symb_re = re.compile(r\"\"\"[!\"#$%&\\'()*+,-./:;<=>?@[\\\\\\]^_`{|}~،؟…«“\\\":\\\"…”]\"\"\")\n",
    "def remove_symbols(text: str) -> str:\n",
    "    return symb_re.sub(repl=\"\", string=text)\n",
    "\n",
    "# 10\n",
    "multiple_space_re = re.compile(\"\\s{2,}\")\n",
    "def remove_multiple_whitespace(text):\n",
    "    return multiple_space_re.sub(repl=\" \", string=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7049ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_list = pd.read_csv('../Data/stop_words/list.csv')['words'].to_list()\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='atbtok', diac=False) # atbseg scheme \n",
    "def text_clean2(txt):\n",
    "    txt = remove_urls(txt)\n",
    "    txt = remove_html(txt)\n",
    "    \n",
    "    # remove stopwords\n",
    "    txt = ' '.join(word for word in txt.split() if word not in stop_word_list)\n",
    "    \n",
    "    # dediacritization\n",
    "    txt = dediac_ar(txt)\n",
    "    \n",
    "    # normalization: Reduce Orthographic Ambiguity and Dialectal Variation\n",
    "    txt = normalize_alef_maksura_ar(txt)\n",
    "    txt = normalize_alef_ar(txt)\n",
    "    txt = normalize_teh_marbuta_ar(txt)\n",
    "    \n",
    "    # normalization: Reducing Morphological Variation\n",
    "    tokens = simple_word_tokenize(txt)\n",
    "    disambig = mle.disambiguate(tokens)\n",
    "    lemmas = [d.analyses[0].analysis['lex'] for d in disambig]\n",
    "    tokens = tokenizer.tokenize(lemmas)\n",
    "    txt = ' '.join(tokens)\n",
    "    \n",
    "    # remove longation\n",
    "#     txt = re.sub(\"[إأآا]\", \"ا\", txt)\n",
    "#     txt = re.sub(\"ى\", \"ي\", txt)\n",
    "#     txt = re.sub(\"ؤ\", \"ء\", txt)\n",
    "#     txt = re.sub(\"ئ\", \"ء\", txt)\n",
    "    txt = re.sub(\"ة\", \"ه\", txt)\n",
    "#     txt = re.sub(\"گ\", \"ك\", txt)\n",
    "    \n",
    "    # remove non-arabic words, or non-numbers, or non-english words in the text\n",
    "    txt = re.sub(r'[^a-zA-Z\\s0-9\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD.0-9]+'\n",
    "                 ,' ', txt)\n",
    "    \n",
    "    # remove symbols\n",
    "    txt = remove_symbols(txt)\n",
    "    \n",
    "    # remove multiple whitespace\n",
    "    txt = remove_multiple_whitespace(txt)\n",
    "    \n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b3c04a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modaj\\OneDrive\\Documents\\Personal\\Jobs\\SHAI\\intern - task 3\\haystack-venv\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\modaj\\OneDrive\\Documents\\Personal\\Jobs\\SHAI\\intern - task 3\\haystack-venv\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning Time: 27089.78033065796 ms\n"
     ]
    }
   ],
   "source": [
    "# apply to your text column\n",
    "start_time = time.time()\n",
    "docs_df_cleaned2 = docs_df.drop(columns=['_id', 'summary', 'content'])\n",
    "docs_df_cleaned2['text_clean'] = docs_df['text'].apply(text_clean2)\n",
    "docs_df_cleaned2['title_clean'] = docs_df['title'].apply(text_clean2)\n",
    "docs_df_cleaned2['content_clean'] = docs_df_cleaned2['title_clean'] + \" \" + docs_df_cleaned2['text_clean']\n",
    "docs_df_cleaned2['doc_id'] = docs_df_cleaned2.index\n",
    "time_measure = (time.time() - start_time) * 10**3\n",
    "print('Data Cleaning Time: {} ms'.format(time_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e55f2854",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "clean2_vect = vectorizer.fit_transform(docs_df_cleaned2['content_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f9f69954",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df_cleaned2['content_word_count'] = docs_df_cleaned2['content_clean'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5b05c9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in corpora: 1144292\n",
      "Number of words in vocabulary 23799\n"
     ]
    }
   ],
   "source": [
    "no_words_corpora = docs_df_cleaned2.content_word_count.sum()\n",
    "# print(f\"number of unique words: {len(clean2_vect.vocabulary_.keys())}\")\n",
    "no_words_vocab = len(vectorizer.vocabulary_.keys())\n",
    "\n",
    "print(f\"Number of words in corpora: {no_words_corpora}\")\n",
    "print(f\"Number of words in vocabulary {no_words_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1d997087",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df_cleaned2 = docs_df_cleaned2.drop(columns=['text_clean', 'title_clean', 'content_word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6287b315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>tags</th>\n",
       "      <th>article_type</th>\n",
       "      <th>text</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>التربية: تحويل 42 مدرسة إلى نظام الفترتين واست...</td>\n",
       "      <td>https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%...</td>\n",
       "      <td>[التربية والتعليم, وزارة التربية والتعليم]</td>\n",
       "      <td>News</td>\n",
       "      <td>أكدت أمين عام  وزارة  التربية والتعليم للشؤون ...</td>\n",
       "      <td>تربيه تحويل 42 مدرسه إلى نظام فتره استئجار 15 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تكريما للمعلمين زيادة منح أبناء المعلمين 550 م...</td>\n",
       "      <td>https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%...</td>\n",
       "      <td>[مكرمة أبناء المعلمين, وزارة التربية والتعليم]</td>\n",
       "      <td>News</td>\n",
       "      <td>احتفلت  وزارة التربية والتعليم  بيوم المعلم بت...</td>\n",
       "      <td>تكريم معلم زياده منح أبناء معلم 550 مقعد إضافي...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الغاز الروسي يضع أوروبا خلال الشتاء المقبل أما...</td>\n",
       "      <td>https://husna.fm/%D9%85%D9%84%D9%81%D8%A7%D8%A...</td>\n",
       "      <td>[الغاز الروسي, أوروبا, أزمة الطاقة]</td>\n",
       "      <td>News</td>\n",
       "      <td>يشهد العالم أول أزمة طاقة عالمية حقيقية في الت...</td>\n",
       "      <td>غاز روسي وضع أوربا شتاء أمام اختبار تاريخي شهد...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>تفاصيل دوام المدارس بعد قرار تثبيت التوقيت الصيفي</td>\n",
       "      <td>https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%...</td>\n",
       "      <td>[التربية والتعليم, التوقيت الصيفي, المدارس]</td>\n",
       "      <td>News</td>\n",
       "      <td>كشفت أمين عام وزارة التربية والتعليم للشؤون ال...</td>\n",
       "      <td>تفصيل دوام مدرسه قرار تثبيت توقيت صيفي كشف أمي...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>العمل: عطلة المولد النبوي الشريف تشمل القطاع ا...</td>\n",
       "      <td>https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%...</td>\n",
       "      <td>[وزارة العمل, المولد النبوي]</td>\n",
       "      <td>News</td>\n",
       "      <td>أكدت  وزارة العمل  أن العطل الرسمية تكون مأجور...</td>\n",
       "      <td>عمل عطله مولد نبوي شريف شمل قطاع خاص أكد وزاره...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  التربية: تحويل 42 مدرسة إلى نظام الفترتين واست...   \n",
       "1  تكريما للمعلمين زيادة منح أبناء المعلمين 550 م...   \n",
       "2  الغاز الروسي يضع أوروبا خلال الشتاء المقبل أما...   \n",
       "3  تفاصيل دوام المدارس بعد قرار تثبيت التوقيت الصيفي   \n",
       "4  العمل: عطلة المولد النبوي الشريف تشمل القطاع ا...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%...   \n",
       "1  https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%...   \n",
       "2  https://husna.fm/%D9%85%D9%84%D9%81%D8%A7%D8%A...   \n",
       "3  https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%...   \n",
       "4  https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%...   \n",
       "\n",
       "                                             tags article_type  \\\n",
       "0      [التربية والتعليم, وزارة التربية والتعليم]         News   \n",
       "1  [مكرمة أبناء المعلمين, وزارة التربية والتعليم]         News   \n",
       "2             [الغاز الروسي, أوروبا, أزمة الطاقة]         News   \n",
       "3     [التربية والتعليم, التوقيت الصيفي, المدارس]         News   \n",
       "4                    [وزارة العمل, المولد النبوي]         News   \n",
       "\n",
       "                                                text  \\\n",
       "0  أكدت أمين عام  وزارة  التربية والتعليم للشؤون ...   \n",
       "1  احتفلت  وزارة التربية والتعليم  بيوم المعلم بت...   \n",
       "2  يشهد العالم أول أزمة طاقة عالمية حقيقية في الت...   \n",
       "3  كشفت أمين عام وزارة التربية والتعليم للشؤون ال...   \n",
       "4  أكدت  وزارة العمل  أن العطل الرسمية تكون مأجور...   \n",
       "\n",
       "                                       content_clean  doc_id  \n",
       "0  تربيه تحويل 42 مدرسه إلى نظام فتره استئجار 15 ...       0  \n",
       "1  تكريم معلم زياده منح أبناء معلم 550 مقعد إضافي...       1  \n",
       "2  غاز روسي وضع أوربا شتاء أمام اختبار تاريخي شهد...       2  \n",
       "3  تفصيل دوام مدرسه قرار تثبيت توقيت صيفي كشف أمي...       3  \n",
       "4  عمل عطله مولد نبوي شريف شمل قطاع خاص أكد وزاره...       4  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df_cleaned2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd1322",
   "metadata": {},
   "source": [
    "### preparing input for word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "87a58bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_list = pd.read_csv('../Data/stop_words/list.csv')['words'].to_list()\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='atbtok', diac=False) # atbseg scheme \n",
    "def text_clean_wv(txt):\n",
    "    txt = remove_urls(txt)\n",
    "    txt = remove_html(txt)\n",
    "    \n",
    "    # remove stopwords\n",
    "    txt = ' '.join(word for word in txt.split() if word not in stop_word_list)\n",
    "    \n",
    "    # dediacritization\n",
    "    txt = dediac_ar(txt)\n",
    "    \n",
    "    # normalization: Reduce Orthographic Ambiguity and Dialectal Variation\n",
    "    txt = normalize_alef_maksura_ar(txt)\n",
    "    txt = normalize_alef_ar(txt)\n",
    "    txt = normalize_teh_marbuta_ar(txt)\n",
    "    \n",
    "    # normalization: Reducing Morphological Variation\n",
    "    tokens = simple_word_tokenize(txt)\n",
    "    disambig = mle.disambiguate(tokens)\n",
    "    lemmas = [d.analyses[0].analysis['lex'] for d in disambig]\n",
    "    tokens = tokenizer.tokenize(lemmas)\n",
    "    txt = ' '.join(tokens)\n",
    "    \n",
    "    # remove longation (EXCLUDED)\n",
    "#     txt = re.sub(\"[إأآا]\", \"ا\", txt)\n",
    "#     txt = re.sub(\"ى\", \"ي\", txt)\n",
    "#     txt = re.sub(\"ؤ\", \"ء\", txt)\n",
    "#     txt = re.sub(\"ئ\", \"ء\", txt)\n",
    "    txt = re.sub(\"ة\", \"ه\", txt)\n",
    "#     txt = re.sub(\"گ\", \"ك\", txt)\n",
    "    \n",
    "    # remove non-arabic words, or non-numbers, or non-english words in the text\n",
    "    txt = re.sub(r'[^a-zA-Z\\s0-9\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD.0-9]+'\n",
    "                 ,' ', txt)\n",
    "    \n",
    "    # remove symbols\n",
    "    txt = remove_symbols(txt)\n",
    "    \n",
    "    # remove multiple whitespace\n",
    "    txt = remove_multiple_whitespace(txt)\n",
    "    \n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "44386051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modaj\\OneDrive\\Documents\\Personal\\Jobs\\SHAI\\intern - task 3\\haystack-venv\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\modaj\\OneDrive\\Documents\\Personal\\Jobs\\SHAI\\intern - task 3\\haystack-venv\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "word_vector_input = docs_df.drop(columns=['_id', 'summary', 'content'])\n",
    "word_vector_input['text_clean'] = docs_df['text'].apply(text_clean_wv)\n",
    "word_vector_input['title_clean'] = docs_df['title'].apply(text_clean_wv)\n",
    "word_vector_input['content_clean'] = word_vector_input['title_clean'] + \" \" + word_vector_input['text_clean']\n",
    "word_vector_input['doc_id'] = word_vector_input.index\n",
    "time_measure = (time.time() - start_time) * 10**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7f77ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_input.to_csv('../Data/processed/SE_data4.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff5721",
   "metadata": {},
   "source": [
    "### Apply Cleaning on Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5b141063",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1 (used cleaning)\n",
    "\n",
    "stop_word_list = pd.read_csv('../Data/stop_words/list.csv')['words'].to_list()\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='atbtok', diac=False) # atbseg scheme \n",
    "def text_clean2_steps(txt):\n",
    "    # remove stopwords\n",
    "    txt = ' '.join(word for word in txt.split() if word not in stop_word_list)\n",
    "    print('stopwords', txt)\n",
    "    \n",
    "    # dediacritization\n",
    "    txt = dediac_ar(txt)\n",
    "    print('dediacritization', txt)\n",
    "    \n",
    "    # normalization: Reduce Orthographic Ambiguity and Dialectal Variation\n",
    "    txt = normalize_alef_maksura_ar(txt)\n",
    "    print('Reduce Orthographic Ambiguity and Dialectal Variation', txt)\n",
    "    txt = normalize_alef_ar(txt)\n",
    "    print('Reduce Orthographic Ambiguity and Dialectal Variation', txt)\n",
    "    txt = normalize_teh_marbuta_ar(txt)\n",
    "    print('Reduce Orthographic Ambiguity and Dialectal Variation', txt)\n",
    "    \n",
    "    # normalization: Reducing Morphological Variation\n",
    "    tokens = simple_word_tokenize(txt)\n",
    "    disambig = mle.disambiguate(tokens)\n",
    "    lemmas = [d.analyses[0].analysis['lex'] for d in disambig]\n",
    "    tokens = tokenizer.tokenize(lemmas)\n",
    "    txt = ' '.join(tokens)\n",
    "    print('Reducing Morphological Variation', txt)\n",
    "    \n",
    "    # remove longation\n",
    "#     txt = re.sub(\"[إأآا]\", \"ا\", txt)\n",
    "#     txt = re.sub(\"ى\", \"ي\", txt)\n",
    "#     txt = re.sub(\"ؤ\", \"ء\", txt)\n",
    "#     txt = re.sub(\"ئ\", \"ء\", txt)\n",
    "    txt = re.sub(\"ة\", \"ه\", txt)\n",
    "#     txt = re.sub(\"گ\", \"ك\", txt)\n",
    "    print('remove longation', txt)\n",
    "    \n",
    "    # remove non-arabic words, or non-numbers, or non-english words in the text\n",
    "    txt = re.sub(r'[^a-zA-Z\\s0-9\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD.0-9]+'\n",
    "                 ,' ', txt)\n",
    "    print('remove non-arabic/non-english/non-number words', txt)\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "55bb3a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords كتاب\n",
      "dediacritization كتاب\n",
      "Reduce Orthographic Ambiguity and Dialectal Variation كتاب\n",
      "Reduce Orthographic Ambiguity and Dialectal Variation كتاب\n",
      "Reduce Orthographic Ambiguity and Dialectal Variation كتاب\n",
      "Reducing Morphological Variation كتاب\n",
      "remove longation كتاب\n",
      "remove non-arabic/non-english/non-number words كتاب\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'كتاب'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_test = 'كتاب'\n",
    "query_test_cleaned = text_clean2_steps(query_test)\n",
    "query_test_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f332e8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "فئة\n"
     ]
    }
   ],
   "source": [
    "# cleaningi with Farasa\n",
    "\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "stemmer = FarasaStemmer()\n",
    "\n",
    "query_test = 'فئات'\n",
    "stemmed_text = stemmer.stem(query_test)                                     \n",
    "print(stemmed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack-venv",
   "language": "python",
   "name": "haystack-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
