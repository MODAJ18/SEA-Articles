{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3f41a6",
   "metadata": {},
   "source": [
    "## 1- Library and Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd3ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# for text cleaning and preprocessing\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ec345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.read_json('../Data/husna.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f9280",
   "metadata": {},
   "source": [
    "## 2- Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3dccb",
   "metadata": {},
   "source": [
    "#### 2.1 preparing data for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5990710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "docs_df = docs_df.drop(columns=['publisher', 'crawled_at', 'url', 'published_at'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6134b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df_dropped = docs_df.drop(index=\n",
    "                               docs_df[(docs_df['content'].str.len() == 0) & (docs_df['title'] == '')].index, axis=0)\n",
    "docs_df_dropped = docs_df_dropped.reset_index(drop=True)\n",
    "docs_df = docs_df_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f3d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df['text'] = docs_df['content'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81137f2",
   "metadata": {},
   "source": [
    "## 3- Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b289ab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_info_text(df_col):\n",
    "    print(f\"-> Number of Documents: {docs_df.shape[0]}\")\n",
    "    print('-' * 50, end='\\n\\n')\n",
    "\n",
    "    print('-> Documents - First 150 letters')\n",
    "    print()\n",
    "    for i, document_i in enumerate(docs_df['text_clean'][:20]):\n",
    "        print(f\"Document Number {i+1}: {document_i[:150]}..\")\n",
    "        print()\n",
    "\n",
    "    print('-' * 50)\n",
    "    \n",
    "def data_preprocessing(df_col):\n",
    "    # Instantiate a TfidfVectorizer object\n",
    "    global vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # It fits the data and transform it as a vector\n",
    "    X = vectorizer.fit_transform(df_col)\n",
    "    # Convert the X as transposed matrix\n",
    "    X = X.T.toarray()\n",
    "    # Create a DataFrame and set the vocabulary as the index\n",
    "    df = pd.DataFrame(X, index=vectorizer.get_feature_names())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340c31f",
   "metadata": {},
   "source": [
    "### 3.1 data cleaning (ver.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392400ab",
   "metadata": {},
   "source": [
    "handle:\n",
    "- removing mentions\n",
    "- removing punctuation\n",
    "- removing Arabic diacritics (short vowels and other harakahs) \n",
    "    - حركات وشد\n",
    "- removing elongation \n",
    "    - مد\n",
    "- removing stopwords (which is available in NLTK corpus)\n",
    "    - normal stopwords (not specific to arabic)\n",
    "- remove words from languages other than arabic and english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4492951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df_cleaned = docs_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "b567140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation symbols\n",
    "punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
    "\n",
    "# Arabic stop words with nltk\n",
    "stop_words = stopwords.words()\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Shadda\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "def clean_text(txt): \n",
    "    #remove punctuations\n",
    "    translator = str.maketrans('', '', punctuations)\n",
    "    txt = txt.translate(translator)\n",
    "    \n",
    "    # remove Tashkeel\n",
    "    txt = re.sub(arabic_diacritics, '', txt)\n",
    "    \n",
    "    # remove longation\n",
    "    txt = re.sub(\"[إأآا]\", \"ا\", txt)\n",
    "    txt = re.sub(\"ى\", \"ي\", txt)\n",
    "    txt = re.sub(\"ؤ\", \"ء\", txt)\n",
    "    txt = re.sub(\"ئ\", \"ء\", txt)\n",
    "    txt = re.sub(\"ة\", \"ه\", txt)\n",
    "    txt = re.sub(\"گ\", \"ك\", txt)\n",
    "    \n",
    "    # remove stopwords\n",
    "    txt = ' '.join(word for word in txt.split() if word not in stop_words)\n",
    "    \n",
    "    # remove non-arabic words, or non-numbers, or non-english words in the text\n",
    "    txt = re.sub(r'[^a-zA-Z\\s0-9\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD.0-9]+'\n",
    "                 ,' ', txt)\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "32cc09c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modaj\\OneDrive\\Documents\\Personal\\Jobs\\SHAI\\intern - task 3\\SEA_venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5074</th>\n",
       "      <th>5075</th>\n",
       "      <th>5076</th>\n",
       "      <th>5077</th>\n",
       "      <th>5078</th>\n",
       "      <th>5079</th>\n",
       "      <th>5080</th>\n",
       "      <th>5081</th>\n",
       "      <th>5082</th>\n",
       "      <th>5083</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000015</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ہر</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ہمارے</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ہولوکاسٹ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ہےکہ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>یقول</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89018 rows × 5084 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "00         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "000        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "0000       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "00000015   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "0001       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "ہر         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "ہمارے      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "ہولوکاسٹ   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "ہےکہ       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "یقول       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "          5074  5075  5076  5077  5078  5079  5080  5081  5082  5083  \n",
       "00         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "000        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "0000       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "00000015   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "0001       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "ہر         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "ہمارے      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "ہولوکاسٹ   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "ہےکہ       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "یقول       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[89018 rows x 5084 columns]"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df_cleaned = docs_df.drop(columns=['_id', 'title', 'summary', 'content', 'text'])\n",
    "\n",
    "start_time = time.time()\n",
    "docs_df_cleaned['text_clean'] = docs_df['text'].apply(clean_text)\n",
    "time_measure = (time.time() - start_time) * 10**3\n",
    "\n",
    "# docs_df_cleaned['summary_clean'] = docs_df['summary'].apply(clean_text) # no need for now\n",
    "docs_df_cleaned['title_clean'] = docs_df['title'].apply(clean_text)\n",
    "text_clean_enc_df = data_preprocessing(docs_df_cleaned['text_clean'])\n",
    "\n",
    "text_clean_enc_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8c63f",
   "metadata": {},
   "source": [
    "**NOTE** \n",
    "- ~6000 source documents -> ~5000 documents -> ~89,000 tokens\n",
    "- Clean time for 'text' column: ~111 seconds\n",
    "- **Problems**:\n",
    "    - may not be normalized enough\n",
    "    - words from other languages\n",
    "    - confusing numbers (remove or keep?)\n",
    "        - remove english numbers? or arabic numbers? or both?\n",
    "        - should we remove words of letters mixed with numbers (E.g. COVID19)\n",
    "    - links (remove or keep?)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f88040c",
   "metadata": {},
   "source": [
    "### 3.1 data cleaning (ver.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a35f97",
   "metadata": {},
   "source": [
    "handle:\n",
    "- removing Arabic diacritics (short vowels and other harakahs)\n",
    "- variation by form and spelling, based on context (Orthographic Ambiguity)\n",
    "- existence of many forms for the same word (Morphological Richness)\n",
    "- dialects (Dialectal Variation)\n",
    "- different ways to write the same word when writing in dialectal Arabic, for which there is no agreed-upon standard\n",
    "    - Orthographic Inconsistency\n",
    "- removing longation and stop words\n",
    "- remove words from languages other than arabic and english\n",
    "   \n",
    "these problems can possibly lead to immensly large vocabularies generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "adfcff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df_cleaned2 = docs_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "31a52569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dediacritization tool\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "# Reducing Orthographic Ambiguity\n",
    "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_ar\n",
    "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
    "\n",
    "# toknenization\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "\n",
    "# Morphological Disambiguation (Maximum Likelihood Disambiguator)\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "mle = MLEDisambiguator.pretrained() # instantiation fo MLE disambiguator\n",
    "\n",
    "# tokenization / lemmatization (choosing approach that best fit the project)\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "7049ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words()\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='atbtok', diac=False) # atbseg scheme \n",
    "def text_clean2(txt):\n",
    "    # dediacritization\n",
    "    txt = dediac_ar(txt)\n",
    "    \n",
    "    # normalization: Reduce Orthographic Ambiguity and Dialectal Variation\n",
    "    txt = normalize_alef_maksura_ar(txt)\n",
    "    txt = normalize_alef_ar(txt)\n",
    "    txt = normalize_teh_marbuta_ar(txt)\n",
    "    \n",
    "    # normalization: Reducing Morphological Variation\n",
    "    tokens = simple_word_tokenize(txt)\n",
    "    disambig = mle.disambiguate(tokens)\n",
    "    lemmas = [d.analyses[0].analysis['lex'] for d in disambig]\n",
    "    tokens = tokenizer.tokenize(lemmas)\n",
    "    txt = ' '.join(tokens)\n",
    "    \n",
    "    # remove longation\n",
    "    txt = re.sub(\"[إأآا]\", \"ا\", txt)\n",
    "    txt = re.sub(\"ى\", \"ي\", txt)\n",
    "    txt = re.sub(\"ؤ\", \"ء\", txt)\n",
    "    txt = re.sub(\"ئ\", \"ء\", txt)\n",
    "    txt = re.sub(\"ة\", \"ه\", txt)\n",
    "    txt = re.sub(\"گ\", \"ك\", txt)\n",
    "    \n",
    "    # remove stopwords\n",
    "    txt = ' '.join(word for word in txt.split() if word not in stop_words)\n",
    "    \n",
    "    # remove non-arabic words, or non-numbers, or non-english words in the text\n",
    "    txt = re.sub(r'[^a-zA-Z\\s0-9\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD.0-9]+'\n",
    "                 ,' ', txt)\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "b3c04a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to your text column\n",
    "# df['text_clean'] = df['text_clean'].apply(dediac_ar)\n",
    "\n",
    "docs_df_cleaned2 = docs_df.drop(columns=['_id', 'title', 'summary', 'content', 'text'])\n",
    "\n",
    "start_time = time.time()\n",
    "# docs_df_cleaned2['text_clean'] = docs_df['text'].apply(dediac_ar)\n",
    "# docs_df_cleaned2['text_clean'] = docs_df_cleaned2['text_clean'].apply(ortho_normalize)\n",
    "# docs_df_cleaned2['text_clean']  = docs_df_cleaned2['text_clean'].apply(simple_word_tokenize)\n",
    "docs_df_cleaned2['text_clean'] = docs_df['text'].apply(text_clean2)\n",
    "time_measure = (time.time() - start_time) * 10**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "c117516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modaj\\OneDrive\\Documents\\Personal\\Jobs\\SHAI\\intern - task 3\\SEA_venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5074</th>\n",
       "      <th>5075</th>\n",
       "      <th>5076</th>\n",
       "      <th>5077</th>\n",
       "      <th>5078</th>\n",
       "      <th>5079</th>\n",
       "      <th>5080</th>\n",
       "      <th>5081</th>\n",
       "      <th>5082</th>\n",
       "      <th>5083</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000015</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ہر</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ہمارے</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ہولوکاسٹ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ہےکہ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>یقول</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23471 rows × 5084 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "00         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "000        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "0000       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "0000015    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "001        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "ہر         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "ہمارے      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "ہولوکاسٹ   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "ہےکہ       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "یقول       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "          5074  5075  5076  5077  5078  5079  5080  5081  5082  5083  \n",
       "00         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "000        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "0000       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "0000015    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "001        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "ہر         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "ہمارے      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "ہولوکاسٹ   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "ہےکہ       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "یقول       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[23471 rows x 5084 columns]"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean_enc_df = data_preprocessing(docs_df_cleaned2['text_clean'])\n",
    "\n",
    "text_clean_enc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13851cf6",
   "metadata": {},
   "source": [
    "- ~6000 source documents -> ~5000 documents -> ~23,000 tokens\n",
    "- Clean time for 'text' column: ~180 seconds\n",
    "- Problems:\n",
    "    - stopwords (remove or keep?)\n",
    "    - normalization may have cut out too many tokens \n",
    "    - confusing numbers (remove or keep?)\n",
    "        - remove english numbers? or arabic numbers? or both?\n",
    "    - should we remove words of letters mixed with numbers (E.g. COVID19)\n",
    "    - links (remove or keep?)\n",
    "    \n",
    "**NOTE** discuss with instructor before proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82637c",
   "metadata": {},
   "source": [
    "### 3.x check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "21919998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(docs_df_cleaned.head())\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "e729eb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> 118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['اجرامي', 'اجراميا', 'اجراميه', 'اجراه', 'اجراها', 'اجرت', 'اجرته',\n",
       "       'اجرتها', 'اجره', 'اجرها', 'اجرهم', 'اجرو', 'اجروا', 'اجري', 'اجرياها',\n",
       "       'اجرياهما', 'اجريت', 'اجرين', 'اجزاء', 'اجزاءه', 'اجزاءها', 'اجزم',\n",
       "       'اجساد', 'اجسادهم', 'اجسام', 'اجساما', 'اجسامنا', 'اجسامهم', 'اجعلك',\n",
       "       'اجل', 'اجلاء', 'اجلاءه', 'اجلاءها', 'اجلاءهم', 'اجلال', 'اجلت', 'اجلس',\n",
       "       'اجله', 'اجلها', 'اجلهم', 'اجلي', 'اجماع', 'اجماعا', 'اجمال', 'اجمالي',\n",
       "       'اجماليه', 'اجمع', 'اجمعت', 'اجمعوا', 'اجمعين'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['ومستوطنوه', 'ومستوطنيه', 'ومستوفيه', 'ومستوي', 'ومستويات', 'ومستوياته',\n",
       "       'ومسجد', 'ومسجدها', 'ومسجل', 'ومسجله', 'ومسددين', 'ومسرحه', 'ومسرحيه',\n",
       "       'ومسرحيين', 'ومسرعه', 'ومسري', 'ومسعر', 'ومسكين', 'ومسلح', 'ومسلحه',\n",
       "       'ومسلسل', 'ومسلسلات', 'ومسلم', 'ومسلمات', 'ومسلمه', 'ومسلمين',\n",
       "       'ومسلوقه', 'ومسمع', 'ومسميات', 'ومسنه', 'ومسنون', 'ومسوحات', 'ومسوده',\n",
       "       'ومسور', 'ومسيء', 'ومسيحين', 'ومسيحيه', 'ومسيحيين', 'ومسيرات',\n",
       "       'ومسيرته', 'ومسيرتهم', 'ومسيره', 'ومشابهه', 'ومشادات', 'ومشارفها',\n",
       "       'ومشاركاتهم', 'ومشاركته', 'ومشاركتهم', 'ومشاركه'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean time: 99.80 seconds\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print(f'--> {i}')\n",
    "display(text_clean_enc_df.index[50*i:50*(i+1)])\n",
    "display(text_clean_enc_df.index[-50*(i+1):(-50*i)-1])\n",
    "i += 1\n",
    "\n",
    "print('clean time: {:.2f} seconds'.format(time_measure * 10**-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890fd90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ = vectorizer.vocabulary_\n",
    "print(f\"number of unique words: {len(vocab_.keys())}\")\n",
    "most_freq_word = sorted(vocab_.items(), key=lambda x: x[1], reverse=True)[:1][0]\n",
    "print('most frequent word is --> {} ({} times)'.format(most_freq_word[0], most_freq_word[1]))\n",
    "score = len(vocab_.keys()) / most_freq_word[1]\n",
    "print('Ratio: {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff5721",
   "metadata": {},
   "source": [
    "## 4- Apply Cleaning on Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "55bb3a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'مولد نبي'"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_test = 'مولد النبي'\n",
    "query_test_cleaned = text_clean2(query_test)\n",
    "query_test_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4533a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "\n",
    "# query_transformed = vectorizer.transform([query_test_cleaned])\n",
    "# query_transformed = query_transformed.T.toarray()\n",
    "# df = pd.DataFrame(query_transformed, index=vectorizer.get_feature_names())\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223a0c0",
   "metadata": {},
   "source": [
    "## 5- Calculating Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "3bf9556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_articles(q, df):\n",
    "    # Convert the query become a vector\n",
    "    q = [q]\n",
    "    q_vec = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
    "    \n",
    "    # Calculate the similarity\n",
    "    sim = {}\n",
    "    for i in range(df.shape[1]):\n",
    "        sim[i] = np.dot(df.loc[:, i].values, q_vec) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vec)\n",
    "        if np.isnan(sim[i]):\n",
    "            sim[i] = 0\n",
    "\n",
    "    # Sort the values \n",
    "    sim_sorted = list(sim.items())\n",
    "    return sim_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "d8c0e1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modaj\\AppData\\Local\\Temp\\ipykernel_30240\\3369844079.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sim[i] = np.dot(df.loc[:, i].values, q_vec) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.0),\n",
       " (1, 0.0),\n",
       " (2, 0.0),\n",
       " (3, 0.0),\n",
       " (4, 0.0),\n",
       " (5, 0.0),\n",
       " (6, 0.0),\n",
       " (7, 0.0),\n",
       " (8, 0.0),\n",
       " (9, 0.0)]"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add The Query\n",
    "q1 = 'مولد النبي'\n",
    "q1_cleaned = text_clean2(query_test)\n",
    "\n",
    "# Measures\n",
    "time_measure = None\n",
    "most_freq_measure = None  \n",
    "\n",
    "start_time = time.time()\n",
    "sorted_docs_with_scores = get_similar_articles(q1, text_clean_enc_df)  # call function\n",
    "time_measure = (time.time() - start_time) * 10**3\n",
    "\n",
    "sorted_docs_with_scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "1a45b9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words: 89018\n",
      "most frequent word is --> یقول (89017 times)\n",
      "Ratio: 1.000\n"
     ]
    }
   ],
   "source": [
    "vocab_ = vectorizer.vocabulary_\n",
    "print(f\"number of unique words: {len(vocab_.keys())}\")\n",
    "most_freq_word = sorted(vocab_.items(), key=lambda x: x[1], reverse=True)[:1][0]\n",
    "print('most frequent word is --> {} ({} times)'.format(most_freq_word[0], most_freq_word[1]))\n",
    "score = len(vocab_.keys()) / most_freq_word[1]\n",
    "print('Ratio: {:.3f}'.format(score))\n",
    "\n",
    "most_freq_measure = most_freq_word[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a122f227",
   "metadata": {},
   "source": [
    "## 6- getting top documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "a648df54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1697, 2152, 2608, 2527, 2147])"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_docs_with_scores = sorted(sorted_docs_with_scores, key=lambda x: x[1], reverse=True)\n",
    "top_5_docs = np.array(sorted_docs_with_scores, dtype='int32')[:5, 0]\n",
    "top_5_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f90667ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time measure: 648.0000019073486\n",
      "frequency measure: 89123\n",
      "score 1.000\n"
     ]
    }
   ],
   "source": [
    "# results \n",
    "print('time measure:', time_measure)\n",
    "print('frequency measure:', most_freq_measure)\n",
    "print('score %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803db5d6",
   "metadata": {},
   "source": [
    "## 7- Organizing Search Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "32957b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df['text_clean'] = docs_df['text'].apply(clean_text)\n",
    "docs_df['summary_clean'] = docs_df['summary'].apply(clean_text)\n",
    "docs_df['title_clean'] = docs_df['title'].apply(clean_text)\n",
    "# + tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d46c319e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>published_at</th>\n",
       "      <th>crawled_at</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>article_type</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>summary_clean</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الزراعة تدرس أفكارا لشركات خاصة لاستثمار مياه ...</td>\n",
       "      <td>https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%...</td>\n",
       "      <td>2021-09-01 06:55:00</td>\n",
       "      <td>2022-10-07 08:32:02.179500</td>\n",
       "      <td>كشف أمين عام وزارة الزراعة محمد الحياري لـ حسن...</td>\n",
       "      <td>[كما أكد الحياري أن التعليمات الجديدة التي ستط...</td>\n",
       "      <td>[مياه الزيبار, وزارة الزراعة, زيت الزيتون]</td>\n",
       "      <td>News</td>\n",
       "      <td>كما أكد الحياري أن التعليمات الجديدة التي ستطب...</td>\n",
       "      <td>اكد الحياري ان التعليمات الجديده ستطبق اعتبارا...</td>\n",
       "      <td>كشف امين عام وزاره الزراعه محمد الحياري حسني ا...</td>\n",
       "      <td>الزراعه تدرس افكارا لشركات خاصه لاستثمار مياه ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  الزراعة تدرس أفكارا لشركات خاصة لاستثمار مياه ...   \n",
       "\n",
       "                                                 url        published_at  \\\n",
       "0  https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%... 2021-09-01 06:55:00   \n",
       "\n",
       "                  crawled_at  \\\n",
       "0 2022-10-07 08:32:02.179500   \n",
       "\n",
       "                                             summary  \\\n",
       "0  كشف أمين عام وزارة الزراعة محمد الحياري لـ حسن...   \n",
       "\n",
       "                                             content  \\\n",
       "0  [كما أكد الحياري أن التعليمات الجديدة التي ستط...   \n",
       "\n",
       "                                         tags article_type  \\\n",
       "0  [مياه الزيبار, وزارة الزراعة, زيت الزيتون]         News   \n",
       "\n",
       "                                                text  \\\n",
       "0  كما أكد الحياري أن التعليمات الجديدة التي ستطب...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  اكد الحياري ان التعليمات الجديده ستطبق اعتبارا...   \n",
       "\n",
       "                                       summary_clean  \\\n",
       "0  كشف امين عام وزاره الزراعه محمد الحياري حسني ا...   \n",
       "\n",
       "                                         title_clean  \n",
       "0  الزراعه تدرس افكارا لشركات خاصه لاستثمار مياه ...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a22b829c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [مياه الزيبار, وزارة الزراعة, زيت الزيتون]\n",
       "1                            [السعودية, الأردن]\n",
       "2                          [ذكرى المولد النبوي]\n",
       "Name: tags, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0458fb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['مياه الزيبار', 'وزارة الزراعة', 'زيت الزيتون']\n",
      "[ True False]\n",
      "[False  True]\n",
      "[False False]\n",
      "['السعودية', 'الأردن']\n",
      "[False False]\n",
      "[False False]\n",
      "['ذكرى المولد النبوي']\n",
      "[False False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 2.0), (1, 0.0), (2, 0.0)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_test = 'مياه الزراعة'\n",
    "doc_ids = list(docs_df['tags'].index)\n",
    "q_list = np.array(q_test.split(' '))\n",
    "sim_score = list(np.zeros(docs_df.shape[0]))\n",
    "\n",
    "for i, tag in enumerate(docs_df['tags']):\n",
    "    print(tag)\n",
    "    for str_tag in tag:\n",
    "        q_list_map = np.vectorize(lambda x: x in str_tag)(q_list) \n",
    "        if True in q_list_map:\n",
    "            sim_score[i] += 1\n",
    "        print(q_list_map)\n",
    "        \n",
    "        \n",
    "sim_non_sorted = list(zip(doc_ids, sim_score))\n",
    "sim_sorted = sorted(sim_non_sorted, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "sim_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0039a79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- FINAL -------------------------\n",
      "resulting simalarities:\n",
      "(0.0, 0.0)\n",
      "(1.0, 0.0)\n",
      "(2.0, 0.15821931459273364)\n",
      "\n",
      "search engine time taken 2.998828887939453 ms\n",
      "search engine average score 1.021043771043771 (uniqueness/frequency)\n",
      "------------------------- ----- -------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modaj\\anaconda3\\envs\\DataEngineering\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\modaj\\anaconda3\\envs\\DataEngineering\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\modaj\\AppData\\Local\\Temp\\ipykernel_143704\\3369844079.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sim[i] = np.dot(df.loc[:, i].values, q_vec) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vec)\n",
      "C:\\Users\\modaj\\anaconda3\\envs\\DataEngineering\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "cols_weights = [0.1, 0.5, 0.15, 0.2]\n",
    "cols = ['tags', 'text_clean', 'summary_clean', 'title_clean']\n",
    "def overall(q, df):\n",
    "    # print(\"query:\", q)\n",
    "    \n",
    "    # note: potential class vars\n",
    "    overall_time = 0\n",
    "    overall_score_measure = 0\n",
    "    similarities_list = []\n",
    "    \n",
    "    # apply search over every col of interest\n",
    "    for col in cols:\n",
    "        if col != 'tags':\n",
    "            # 1 data preprocessing(each column)\n",
    "            text_clean_enc_df = data_preprocessing(docs_df[col])\n",
    "\n",
    "            # 2- count similarities (each column)\n",
    "            time_measure = None\n",
    "            most_freq_measure = None  \n",
    "            start_time = time.time()\n",
    "            sorted_docs_with_scores_content = get_similar_articles(q, text_clean_enc_df)  # call function\n",
    "            time_measure = (time.time() - start_time) * 10**3\n",
    "\n",
    "            # 3- results\n",
    "            global vectorizer\n",
    "            vocab_ = vectorizer.vocabulary_\n",
    "            # print(f\"number of unique words: {len(vocab_.keys())}\")\n",
    "            most_freq_word = sorted(vocab_.items(), key=lambda x: x[1], reverse=True)[:1][0]\n",
    "            # print('most frequent word is --> {} ({} times)'.format(most_freq_word[0], most_freq_word[1]))\n",
    "\n",
    "            score = len(vocab_.keys()) / most_freq_word[1]\n",
    "            # print('Ratio: {:.3f}'.format(score))\n",
    "            # print()\n",
    "            # print('time measure:', time_measure)\n",
    "        else:\n",
    "            time_measure = None\n",
    "            most_freq_measure = None  \n",
    "            start_time = time.time()\n",
    "            \n",
    "            doc_ids = list(docs_df['tags'].index)\n",
    "            q_list = np.array(q.split(' '))\n",
    "            sim_score = list(np.zeros(docs_df.shape[0]))\n",
    "\n",
    "            for i, tag in enumerate(docs_df['tags']):\n",
    "                # print(tag)\n",
    "                for str_tag in tag:\n",
    "                    q_list_map = np.vectorize(lambda x: x in str_tag)(q_list) \n",
    "                    if True in q_list_map:\n",
    "                        sim_score[i] += 1\n",
    "                    # print(q_list_map)\n",
    "\n",
    "            sim_non_sorted = list(zip(doc_ids, sim_score))\n",
    "            sorted_docs_with_scores_content = sim_non_sorted\n",
    "            \n",
    "            time_measure = (time.time() - start_time) * 10**3\n",
    "            score = 0\n",
    "            \n",
    "        similarities_list.append(sorted_docs_with_scores_content)\n",
    "        overall_time += time_measure\n",
    "        overall_score_measure += (score/(len(cols) - 1))\n",
    "        \n",
    "        averaged_scores_ids = np.array(resulting_simalarities)[0, :, 0]\n",
    "        averaged_scores = np.average(np.array(resulting_simalarities)[:, :, 1], axis=0, weights=cols_weights)\n",
    "        similarities_scores = list(zip(averaged_scores_ids, averaged_scores))\n",
    "        # print('--------------------')\n",
    "    \n",
    "    return similarities_scores, overall_time, overall_score_measure\n",
    "    \n",
    "    \n",
    "q1 = 'مولد النبي'\n",
    "resulting_simalarities, SE_time, SE_avg_score = overall(q1, docs_df)\n",
    "\n",
    "print(\"-\" * 25, 'FINAL', '-' * 25)\n",
    "print('resulting simalarities:')\n",
    "for rs in resulting_simalarities:\n",
    "    print(rs)\n",
    "print()\n",
    "print('search engine time taken', SE_time, 'ms')\n",
    "print('search engine average score', SE_avg_score, '(uniqueness/frequency)')\n",
    "print(\"-\" * 25, '-----', '-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ef326a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.       , 0.       , 1.       ],\n",
       "       [0.       , 0.       , 0.1006167],\n",
       "       [0.       , 0.       , 0.       ],\n",
       "       [0.       , 0.       , 0.       ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.15821931])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0), (1.0, 0.0), (2.0, 0.15821931459273364)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST: COMPUTING AVERAGE SIMILARITY SCORES\n",
    "# averaged_scores_ids = np.array(resulting_simalarities)[0, :, 0]\n",
    "# display(averaged_scores_ids)\n",
    "\n",
    "# display(np.array(resulting_simalarities)[:, :, 1])\n",
    "\n",
    "# averaged_scores = np.average(np.array(resulting_simalarities)[:, :, 1], axis=0, weights=cols_weights)\n",
    "# display(averaged_scores)\n",
    "\n",
    "# list(zip(averaged_scores_ids, averaged_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SEA",
   "language": "python",
   "name": "sea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
